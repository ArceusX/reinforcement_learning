{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Series Seminar: Reinforcement Learning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reinforcement Learning](http://en.wikipedia.org/wiki/Reinforcement_learning) is concerned with finding the optimal <i>action</i> to take for any given <i>state</i> an agent may encounter in an <i>environment</i> in order to maximize some <i>reward</i> value.  These state-action mappings are called <i>policies</i>.  The key difference from supervised learning is that for each state we are not told the correct action, instead we are only given the cumulative reward after some evaluation period (delayed reward).  For example, consider a learning to play soccer.  The reward signal is: Did we win?, however there many actions taken by players during the game that could have contributed to winning, losing, or not at all. How much did scoring that goal, or making that pass, or moving to that location contribute to winning?  See [Sutton & Barto](http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html) for a full introduction to reinforcement learning.\n",
    "\n",
    "For very small state and action spaces, these policies can all be evaluated through <i>brute force</i>; however, for almost any interesting domain, the state and action space is going to be so large that the cost of testing all possible policies will be prohibitively high.  Thus reinforcement learning focuses on creating algorithms to find these policies without resorting to exhaustive search.\n",
    "\n",
    "The first approach to be explored in this module is Temporal Difference Learning (TD-Learning), a variant of <i>value-function learning</i>.  Briefly, such approaches focus on learning the reward structure of the domain, that is, a policy is continually evaluated against the domain to determine what rewards particular state-action mappings receive and then the policy is updated to maximize the cumulative reward as determined by the learned reward structure.  Such approaches are often based upon [Markov Decision Process](http://en.wikipedia.org/wiki/Markov_decision_process) theory and rely on the related underlying assumptions, such as the [Markov property](http://en.wikipedia.org/wiki/Markov_property).\n",
    "\n",
    "The second approach in this module is a type of <i>direct policy search</i>.  In policy search, a stochastic optimization approach is applied directly on policies in order to find the optimal ones.  These approaches fall into either gradient-based (optimize the policy based upon gradient information extracted through evaluations) or gradient-free. In particular, this module will examine the gradient-free approach [Evolutionary Computation](http://en.wikipedia.org/wiki/Evolutionary_computation), inspired by the theory of natural selection. \n",
    "\n",
    "The next section discusses the TD-Learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Difference (TD) Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key characteristic of value-function learning approaches, such as TD-Learning, is that they naturally consider the entire learning problem in an online decision-making framework.  That is, this form of reinforcement learning is concerned with how the learning agent interacts with the environment, what the learning agent can measure from the environment, what is the ultimate goal of the learning agent, and how to deal with uncertainty. Reinforcement learning of this type closely resembles learning observed in nature wherein it is rare, if even possible, to have a teacher available who can provide the <i>right</i> actions for a learner to follow in <i>every</i> possible state. One of the unique challenges of this learning is the balance between <i>exploration</i> and <i>exploitation</i>, that is, the agent has to <i>exploit</i> what it already knows in order to obtain a reward, but it also has to <i>explore</i> in order to find actions that yield better rewards. \n",
    "\n",
    "To illustrate the need to balance exploration and exploitation, consider a gambler presented with a number of slot machines.  Each slot machine has a different parameters for their payout rates that are unknown to the gambler.  Most of the machines cause the gambler to lose money, some let the gamble break even, and a few allow the gambler to win.  If the gambler plays all the machines, he will lose on average.  If the gambler focuses only on the first machine that seems to payout, the optimal machine may be ignored.  Thus the gambler needs to expend some time to try different machines in order to model their rewards, but must also accumulate as much reward as possible with the time/money they have.  This problem is known as the <i>Multi-armed Bandit</i> problem. \n",
    "\n",
    "TD-Learning is described in the framework of Markov Decision Processes (MDP), which is a strong theoretical model for decision making in situations where the transition of the environment is stochastic. An agent with full knowledge of the MDP can seek for an optimal policy that maximizes some function of its expected cumulative reward. If the MDP is unknown, then an agent learns to take optimal actions through interactions with the environment alone.  MDPs are described next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Markov Decision Processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential decision making problems are often cast as a Markov Decision Process (MDP). An MDP is comprised by a tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R})$ where $\\mathcal{S}$ denotes the set of states, $\\mathcal{A}$ the set of actions, $\\mathcal{P}$ a set of transition probabilities $P_{s,s'}^a:=Pr(s'|s,a)$ each representing the probability of transitioning from $s\\in\\mathcal{S}$ to $s'\\in\\mathcal{S}$ after taking action $a\\in\\mathcal{A}$, and $\\mathcal{R}$ a set of rewards $r_{s,s'}^a$ each representing the one-step reward obtained when transitioning from $s$ to $s'$ after taking $a$. \n",
    "\n",
    "Note that MDPs are underpinned by several assumptions.  On such property is that the transition probabilities for a particular action rely only on the current state, i.e.:\n",
    "\n",
    "$$\n",
    "Pr(s_{t+1}=s'~|~s_t=s,a_t=a)=Pr(s_{t+1}=s'~|~s_t=s,a_t=a,s_{t-1}=s,a_{t-1}=a,\\ldots,s_0=s,a_0=a)\n",
    "$$\n",
    "\n",
    "That is, the system is <i>memoryless</i>, also known as the <i>Markov property</i>, thus an agent can predict the next state and expected reward of the MDP based on the current state $s$ and action $a$ alone. Hence, the name Markov Decision Process.  Another assumption is that the world is <i>stationary</i>, that is, the system model does not change.  Finally, for MDPs a common assumption is full observability, meaning the entire state of the world can be observed at each time step.  A looser restriction on this assumption results in what is known as a Partially Observable Markov Decision Process (POMDP), which represent a great deal of real-world problems.\n",
    "\n",
    "A trajectory is a sequence $\\{s_0,a_0,r_0,s_1,a_1,r_1,\\ldots\\}$, where $r_t:=r^{a_t}_{s_t,s_{t+1}}$ and $s_t$ ($a_t$) denotes the state (action) observed (taken) at time $t$. Each action $a_t\\in\\mathcal{A}$ in the trajectory is chosen according to a policy $\\pi:\\mathcal{S}\\rightarrow \\mathcal{A}$, which maps each state to a given action. Although this module focuses on deterministic policies, it is important to remark that probabilistic policies can also be developed where the policy yields a probability distribution over $\\mathcal{A}$. \n",
    "\n",
    "Given a policy $\\pi$, the state-action value function $Q^{\\pi}(s,a)$ for each state-action pair is \n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s,a):=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^tr_t ~|~s_0=s,a_0=a\\right]\n",
    "$$\n",
    "\n",
    "where the discount factor $\\gamma\\in(0,1)$ down weights the value of future rewards, and $\\mathbb{E}_{\\pi}$ denotes the expected value with respect to the state given that the agent follows policy $\\pi$. The state-action value function quantifies how good it is for the learning agent to take action $a$ when in state $s$. Similarly, the state value function for a given $\\pi$ is\n",
    "\n",
    "\\begin{align}\n",
    "V^{\\pi}(s)&:=\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^tr_t ~|~s_0=s\\right]\\\\\n",
    "&=Q^{\\pi}(s,\\pi(s))\n",
    "\\end{align}\n",
    "\n",
    "which quantifies how good it is for the agent to be in state $s$. A subtle difference between $V^{\\pi}(s)$ and $Q^{\\pi}(s,a)$ is that $\\forall t\\geq0$, $a_t$ is chosen according to $\\pi$ in the former, and $a_t$ is chosen accordingly to $\\pi$ $\\forall t>0$ with $a_0$ fixed as $a_0=a$ in the latter. The state value function can be written recursively as $V^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[r_{s,s'}^{\\pi(s)}+\\gamma V^{\\pi}(s')\\right]$. This expression relates the value of being at a given state $s$ to that of being at successor states $s'\\in\\mathcal{S}$. \n",
    "\n",
    "The planning problem for an MDP is, given the tuple $(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R})$ find an optimal policy $\\pi^*(s)$ that maximizes the expected cumulative discounted rewards\n",
    "\n",
    "$$\n",
    "\\pi^*(s)=\\mathop{\\arg\\max}_{\\pi}~~\\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty}\\gamma^tr_t ~|~s_0=s\\right].\n",
    "$$\n",
    "\n",
    "Note that $\\pi^*$ depends on a given initial state $s$. The optimal value function $V^*(s):=V^{\\pi^*}(s)$ satisfies the Bellman optimality equation\n",
    "\n",
    "\\begin{align}\n",
    "V^*(s) &=\\max_{a\\in\\mathcal{A}}~\\mathbb{E}_{\\pi^*}\\left[r_{s,s'}^a+\\gamma V^*(s')\\right]\\\\\n",
    "&=\\max_{a\\in\\mathcal{A}}~\\sum_{s'\\in\\mathcal{S}}P_{s,s'}^a\\left[r_{s,s'}^a+\\gamma V^*(s')\\right].\n",
    "\\end{align}\n",
    " \n",
    " Given a well-defined MDP, it is possible to directly solve for the optimal policy, as described next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Key Terms</b>\n",
    "   <ul><li><b>Environment</b>:        World with which a learning agent interacts</li>\n",
    "       <li><b>State ($s$)</b>:        Observations that define the environment for the agent</li>\n",
    "       <li><b>State-Space ($S$)</b>:  All possible values of the state</li>\n",
    "       <li><b>Action ($a$)</b>:       Decision by agent that interacts with the environment</li>\n",
    "       <li><b>Action-Space ($A$)</b>: All possible actions the agent may make</li>\n",
    "       <li><b>State Transition:</b>   Moving from one state to another state given an action</li>\n",
    "       <li><b>Policy ($\\pi$)</b>:     Mapping of states to actions ($\\pi(s)=a$)</li>\n",
    "       <li><b>Reward ($r$)</b>:       Benefit received from a particular state</li>\n",
    "   <ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Directly Solving the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming is an iterative approach for solving sequential decision-making problems. It rests on the principle of optimality which states that an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. An instantiation of dynamic programming can be used to solve the planning problem for an MDP by finding the optimal state value function and its corresponding optimal policy. The state value function $V^{\\pi}(s)$ can be written recursively as a function of the underlying policy $\\pi$ as\n",
    "\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) &=\\sum_{s'\\in\\mathcal{S}}P_{s,s'}^{\\pi(s)}\\left[r_{s,s'}^{\\pi(s)}+\\gamma V^{\\pi}(s')\\right],~~\\forall s\\in\\mathcal{S}.\n",
    "\\end{align}\n",
    "\n",
    "Note that the latter set of equations is different from the Bellman optimality equations in that it is policy dependent.\n",
    "\n",
    "When $\\mathcal{S}:=\\{s_1,\\ldots,s_{|\\mathcal{S}|}\\}$ is finite, the state value functions can be calculated in closed form by solving $|\\mathcal{S}|$ linear equations, one for each state of the MDP. Let $\\mathbf{P}\\in\\mathbb{R}^{|\\mathcal{S}|\\times |\\mathcal{S}|}$ denote the matrix of transition probabilities where its $(i,j)$ entry is given by $\\mathbf{P}_{i,j}:=P_{s_i,s_j}^{\\pi(s_i)}$, $\\mathbf{v}^{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|}$ the vector of value functions of the policy $\\pi$, and $\\mathbf{r}\\in\\mathbb{R}^{|\\mathcal{S}|}$ the vector of expected rewards with entries $\\mathbf{r}_i:=\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^{\\pi(s_i)}r_{s_i,s_j}^{\\pi(s_i)}$. Then, the value function can be readily written as $\\mathbf{v}=\\mathbf{r}+\\gamma\\mathbf{Pv}$ and be written in closed form as\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{v}}=(\\mathbf{I}-\\gamma\\mathbf{P})^{-1}\\mathbf{r}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is a $|\\mathcal{S}|\\times |\\mathcal{S}|$ identity matrix and the inverse $(\\mathbf{I}-\\gamma\\mathbf{P})^{-1}$ always exists. However, computing $\\hat{\\mathbf{v}}$ directly can be computational prohibited, specially for large $|\\mathcal{S}|$. Instead, a fixed-point iteration can be used to iteratively obtain $\\hat{\\mathbf{v}}$ as\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^{(\\tau+1)}\\leftarrow \\mathbf{r}+\\gamma\\mathbf{P}\\mathbf{v}^{(\\tau)}\n",
    "$$\n",
    "\n",
    "where $\\tau$ denotes an iteration index. This fixed-point iteration is guaranteed to converge to $\\hat{\\mathbf{v}}$ as $\\tau\\rightarrow\\infty$. The policy improvement step is done by using the Bellman optimality equations and $\\hat{\\mathbf{v}}$ to select an action greedily as\n",
    "\n",
    "$$\n",
    "\\pi(s_i) =\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}~\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^a\\left[r_{s_i,s_j}^a+\\gamma \\hat{\\mathbf{v}}_j\\right], ~~\\forall s_i\\in\\mathcal{S}.\n",
    "$$\n",
    "\n",
    "Performing iteratively the policy evaluation and policy improvement steps yields the <i>Policy Iteration</i> algorithm. Although Policy Iteration is guaranteed to converge to the optimal solution of the Bellman optimality equations in polynomial time, it is not scalable due to its elevated computational and storage complexity given by $O(|\\mathcal{S}|^3)$ and $O(|\\mathcal{S}|^2)$, respectively.\n",
    "\n",
    "Convergence to the optimal policy can still be guaranteed if the policy improvement step is performed before obtaining $\\hat{\\mathbf{v}}$ exactly as long as ${\\mathbf{v}}$ is improved after each policy evaluation step. Thus, the policy improvement step whould use the best possible action for evaluation rather than a fixed policy. The resulting algorithm, known as <i>Value Iteration</i>, comprises the following updates\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{v}}_i &=\\max_{a\\in\\mathcal{A}}~\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^a\\left[r_{s_i,s_j}^a+\\gamma \\tilde{\\mathbf{v}}_j\\right], ~~\\forall s_i\\in\\mathcal{S}\\\\\n",
    "\\pi(s_i) &=\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}~\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^a\\left[r_{s_i,s_j}^a+\\gamma \\tilde{\\mathbf{v}}_j\\right], ~~\\forall s_i\\in\\mathcal{S}.\n",
    "\\end{align}\n",
    "\n",
    "Value Iteration improves the policy much more frequently and reduces the per-iteration complexity of the Policy Iteration algorithm. In practice Value Iteration often requires less time to find an optimal policy when compared to the Policy Iteration algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how Value Iteration solves these MDP problems, we introduce a grid world domain below.  In this domain, the agent lives in a 3 $\\times$ 4 grid, with the state of the agent defined as the $(x,y)$ coordinate in the grid.  The agent can take the actions of moving Up, Down, Left, or Right.  When taking an action the agent has a probability of either the action succeeding or failing and moving perpendicular to the desired direction.  The grid world consists of squares that are either Open, Blocked, Win, or Lose and each has an associated amount of reward.  The grid world ends when either the Win or Lose end states are entered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Defining the World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set-up the pylab context\n",
    "%pylab inline\n",
    "random.seed(0)\n",
    "# Import libraries that will be used\n",
    "import copy\n",
    "import math\n",
    "from IPython.html.widgets import interact, interactive, fixed\n",
    "from IPython.html import widgets\n",
    "from IPython.display import clear_output, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a 3 X 4 Grid World represented by a 3 x 4 array\n",
    "# Each array location contains a character representing:\n",
    "#    O = Open location for the agent to traverse\n",
    "#    B = Location that cannot be traversed by the agent \n",
    "#    W = Ending location with the agent winning\n",
    "#    L = Ending location with the agent losing\n",
    "world = [['O','O','O','W'],  \n",
    "         ['O','B','O','L'],\n",
    "         ['O','O','O','O']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Setting the Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mapping of current state to reward value\n",
    "rewards = {}\n",
    "\n",
    "# Positive reward for entering the win state\n",
    "rewards['W'] = 1.0 \n",
    "\n",
    "# Negative reward for entering lost state\n",
    "rewards['L'] = -1.0 \n",
    "\n",
    "# No reward for all other states\n",
    "rewards['O'] = 0.0 \n",
    "rewards['B'] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Initializing the Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Value function that defines the utility of being \n",
    "# in a particular state in the 3 X 4 grid world, \n",
    "# initialized to zero\n",
    "utility = [[0.0,0.0,0.0,0.0],\n",
    "           [0.0,0.0,0.0,0.0],\n",
    "           [0.0,0.0,0.0,0.0]]\n",
    "\n",
    "# Function to reset all utility values to 0\n",
    "def reset_util(utility):\n",
    "    for xx in xrange(len(utility)):\n",
    "        for yy in xrange(len(utility[xx])):\n",
    "            utility[xx][yy] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Updating the Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the value function is done through the equation:\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{v}}_{s_i} &= r_{s_i} + \\gamma\\max_{a\\in\\mathcal{A}}~\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^a\\left[\\tilde{\\mathbf{v}}_j\\right], ~~\\forall s_i\\in\\mathcal{S}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discount factor (gamma) applied to future rewards\n",
    "# Smaller value indicates less importance\n",
    "discount_factor = 1.0\n",
    "\n",
    "# Updates the utility of a particular grid location given \n",
    "#   x,y-coordinate in the world, \n",
    "#   current utility values, \n",
    "#   world representation, \n",
    "#   and the rewards\n",
    "def update_utility(xx, yy, utility, world):\n",
    "    \n",
    "    # Determine the reward for this grid location\n",
    "    reward = rewards[world[xx][yy]]\n",
    "        \n",
    "    # If the world location is blocked, it cannot be visited and has 0 utility\n",
    "    if(world[xx][yy] == 'B'):\n",
    "        return 0.0\n",
    "    \n",
    "    # If the world location is either the winning or losing, its utility is its reward\n",
    "    if(world[xx][yy] == 'W' or world[xx][yy] == 'L'):\n",
    "        return reward\n",
    "    \n",
    "    # Else, the utility is the reward of the current location plus the discounted maximum utility from this location \n",
    "    return reward + discount_factor * max_utility(xx, yy, utility, world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to update the complete value-function, we create a function that loops over all possible states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updates the utility of each grid location \n",
    "# given the current utility values and the world\n",
    "def update_all_utility(utility, world):\n",
    "    for xx in range(3):\n",
    "        for yy in range(4):\n",
    "            utility[xx][yy] = update_utility(xx, yy, utility, world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Determining the Action with Max Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below solves for the part of the equation:\n",
    "\\begin{align}\n",
    "\\max_{a\\in\\mathcal{A}}~\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^a\\left[\\tilde{\\mathbf{v}}_j\\right], ~~\\forall s_i\\in\\mathcal{S}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In the grid world, the probability of success \n",
    "# indicates the chance the movement (e.g. Up)\n",
    "# actually moves up, with a failure resulting\n",
    "# in a perpendicular direction (e.g. Left or Right)\n",
    "prob_action_success = 0.8 \n",
    "\n",
    "# Gets the max utility from the current location in the world\n",
    "# By observing the utility of each of the action posibilities\n",
    "# (Up, Down, Left, Right) and selecting the max\n",
    "def max_utility(xx, yy, utility, world):\n",
    "    \n",
    "    # Get the utility from moving up\n",
    "    # which inludes failures going left and right\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    up_utility = prob_action_success * action_utility(xx, yy, -1, 0, utility, world) \n",
    "    up_utility = up_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, -1, utility, world)\n",
    "    up_utility = up_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, 1, utility, world)\n",
    "    \n",
    "    # Get the utility from moving down\n",
    "    # which inludes failures going left and right\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    down_utility = prob_action_success * action_utility(xx, yy, 1, 0, utility, world) \n",
    "    down_utility = down_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, -1, utility, world)\n",
    "    down_utility = down_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, 1, utility, world)\n",
    "    \n",
    "    # Get the utility from moving left\n",
    "    # which inludes failures going up and down\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    left_utility = prob_action_success * action_utility(xx, yy, 0, -1, utility, world) \n",
    "    left_utility = left_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, -1, 0, utility, world)\n",
    "    left_utility = left_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 1, 0, utility, world)\n",
    "    \n",
    "    # Get the utility from moving right\n",
    "    # which inludes failures going up and down\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    right_utility = prob_action_success * action_utility(xx, yy, 0, 1, utility, world) \n",
    "    right_utility = right_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, -1, 0, utility, world)\n",
    "    right_utility = right_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 1, 0, utility, world)\n",
    "    \n",
    "    # Return the max of these utilities\n",
    "    return max(up_utility, down_utility, right_utility, left_utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function is implemented here to address certain edge cases (e.g. moving left from the left most of the world, attempting to move to a blocked grid location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the utility of the new state \n",
    "# given a successful action taken from \n",
    "# the current state \n",
    "def action_utility(xx, yy, xxd, yyd, utility, world):\n",
    "    \n",
    "    # Edge cases where the action would move\n",
    "    # outside the grid world results in staying\n",
    "    # in the current location\n",
    "    if(xx + xxd < 0 or xx + xxd > 2 or yy + yyd < 0 or yy + yyd > 3):\n",
    "        return utility[xx][yy]\n",
    "    \n",
    "    # Edge cases where the action would move\n",
    "    # to a blocked location results in staying\n",
    "    # at the current location\n",
    "    if(world[xx + xxd][yy + yyd] == 'B'):\n",
    "        return utility[xx][yy]\n",
    "    \n",
    "    # Normal case, action moves to new state\n",
    "    # and utility is from the new state\n",
    "    return utility[xx + xxd][yy + yyd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6. Creating Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Policy mapping defines an action\n",
    "# to be taken for each state in the\n",
    "# environment\n",
    "# In the grid world, these actions are to move:\n",
    "#   U = Up \n",
    "#   D = Down\n",
    "#   L = Left, \n",
    "#   R = Right, \n",
    "#   N = Not Applicable (Invalid/End States)\n",
    "policy = [['U', 'U', 'U', 'N'],\n",
    "          ['U', 'N', 'U', 'N'],\n",
    "          ['U', 'U', 'U', 'U']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the policy according to the equation: \n",
    "$$\n",
    "\\pi(s_i) =\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}~\\sum_{j=1}^{|\\mathcal{S}|}P_{s_i,s_j}^a\\left[\\hat{\\mathbf{v}}_j\\right], ~~\\forall s_i\\in\\mathcal{S}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the best action to take\n",
    "# from the current state\n",
    "# given the current value-function and world\n",
    "def best_action(xx, yy, utility, world):\n",
    "    \n",
    "    # No applicable action if the state puts us in a blocked\n",
    "    # or ending position\n",
    "    if(world[xx][yy] == 'B' or world[xx][yy] == 'W' or world[xx][yy] == 'L' ):\n",
    "        return 'N'\n",
    "    \n",
    "    # Get the utility from moving up\n",
    "    # which inludes failures going left and right\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    up_utility = prob_action_success * action_utility(xx, yy, -1, 0, utility, world)\n",
    "    up_utility = up_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, -1, utility, world)\n",
    "    up_utility = up_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, 1, utility, world)\n",
    "    \n",
    "    # Get the utility from moving down\n",
    "    # which inludes failures going left and right\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    down_utility = prob_action_success * action_utility(xx, yy, 1, 0, utility, world) \n",
    "    down_utility = down_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, -1, utility, world)\n",
    "    down_utility = down_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 0, 1, utility, world)\n",
    "\n",
    "    # Get the utility from moving left\n",
    "    # which inludes failures going up and down\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    left_utility = prob_action_success * action_utility(xx, yy, 0, -1, utility, world) \n",
    "    left_utility = left_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, -1, 0, utility, world)\n",
    "    left_utility = left_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 1, 0, utility, world)\n",
    "    \n",
    "    # Get the utility from moving right\n",
    "    # which inludes failures going up and down\n",
    "    # Any other state contributes 0 to the utility\n",
    "    # Because they have probability of 0\n",
    "    right_utility = prob_action_success * action_utility(xx, yy, 0, 1, utility, world) \n",
    "    right_utility = right_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, -1, 0, utility, world)\n",
    "    right_utility = right_utility + (1 - prob_action_success)/2 * action_utility(xx, yy, 1, 0, utility, world)\n",
    "\n",
    "    # Initialize the best action\n",
    "    # to be the first action (Up)\n",
    "    best_action = 'U'\n",
    "    \n",
    "    # Set the maximum utility\n",
    "    # from any action to up's utility\n",
    "    max_utility = up_utility\n",
    "    \n",
    "    # If moving down has more utility\n",
    "    if(down_utility > max_utility):\n",
    "        \n",
    "        # Set the best action to Down\n",
    "        max_utility = down_utility\n",
    "        best_action = 'D'\n",
    "    \n",
    "    # If moving left has more utility\n",
    "    if(left_utility > max_utility):\n",
    "        \n",
    "        # Set the best action to Left\n",
    "        max_utility = left_utility\n",
    "        best_action = 'L'\n",
    "    \n",
    "    #If moving right has the best utility\n",
    "    if(right_utility > max_utility):\n",
    "        # Set the best action to Right\n",
    "        max_utility = right_utility\n",
    "        best_action = 'R'\n",
    "        \n",
    "    # Return the action that maximizes utility\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to sweep across all location, we create a fuction that will loop over the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updates the policy with the actions \n",
    "# that provide the best utility for each state\n",
    "def update_policy(policy, world, utility):\n",
    "    for xx in range(3):\n",
    "        for yy in range(4):\n",
    "            policy[xx][yy] = best_action(xx, yy, utility, world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.7. Running the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing together the parts of the algorithm, a function that performs value iteration for a number of iterations, updating the value function and the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Performs a number of iterations\n",
    "# of the value iteration algorithm\n",
    "def value_iteration(iterations):\n",
    "    \n",
    "    # For the number of iterations\n",
    "    for xx in range(iterations):\n",
    "        \n",
    "        # Update the value function\n",
    "        update_all_utility(utility, world)\n",
    "        # Update the policy\n",
    "        update_policy(policy, world, utility)\n",
    "    \n",
    "    # Print out the value function\n",
    "    for xx in range(3):\n",
    "        print utility[xx]\n",
    "    print\n",
    "\n",
    "    # Print out the policy\n",
    "    for xx in range(3):\n",
    "        print policy[xx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now examine how the algorithm updates the value function and policy by varying the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will reset the utility values\n",
    "# and then perform a number of iterations of\n",
    "# the value iteration approach\n",
    "def interactive_iteration(iterations=1):\n",
    "    reset_util(utility)\n",
    "    value_iteration(iterations)\n",
    "\n",
    "# Create an interactive slider for \n",
    "# the number of iterations\n",
    "interact(interactive_iteration, iterations=(0, 100, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.8. Playing with Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several parameters in this approach that influence the final policy based upon their values.  Among these are the reward values (both the final reward states and the reward for non-final states).  The reward for the final states specifies the ending reward value received for finishing the task in the specified state.  The non-final state rewards represent intermediate rewards during the task and can be applied to influence the policies in different ways.  For example, setting the non-final state reward ('O') to a small negative value causes the policy to attempt to move to the final reward states more quickly.  Experiment with these values below to see how they can significantly change policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will reset the utility values\n",
    "# and then perform the value iteration \n",
    "# approach with the parameters\n",
    "def interactive_iteration(winning_reward=1.0,\n",
    "                          losing_reward=-1.0,\n",
    "                          intermediate_reward=0.0,\n",
    "                          iterations=1000):\n",
    "    # Reward for entering the win state\n",
    "    rewards['W'] = winning_reward  \n",
    "    # Reward for entering the lose state\n",
    "    rewards['L'] = losing_reward\n",
    "    # Reward for entering an open location\n",
    "    rewards['O'] = intermediate_reward\n",
    "    reset_util(utility)\n",
    "    value_iteration(iterations)\n",
    "    \n",
    "# Create interactive sliders for \n",
    "# the rward values\n",
    "interact(interactive_iteration, winning_reward=(0.0, 1.0, 0.05),\n",
    "                                losing_reward=(-1.0, 0.0, 0.05),\n",
    "                                intermediate_reward=(-1.0, 1.0, 0.05),\n",
    "                                iterations=(0,1000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discount Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter that can significantly change the policy is the <i>discount factor</i>.  This parameter controls how distant into the future the policy cares about rewards.  Looking into the future is an important component of such learning because it deals with the <i>credit assignment problem</i>.  The credit assignment problem is the challenge posed by the delayed rewards of a reinforcement learning algorithm that causes difficulty in <i>assigning</i> the <i>credit</i> for the given reward to the correct actions that contributed to receiving it.  A discount factor of one means that there is an infinite horizon on the rewards, that is, the policy cares about all future rewards.  A discount factor of zero means that only immediate rewards matter.  Experiment with this parameter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will reset the utility values\n",
    "# and then perform the value iteration \n",
    "# approach with the parameters\n",
    "def interactive_iteration(discount=1.0,\n",
    "                          winning_reward=1.0,\n",
    "                          losing_reward=-1.0,\n",
    "                          intermediate_reward=0.0,\n",
    "                          iterations=1000):\n",
    "    # Reward for entering the win state\n",
    "    rewards['W'] = winning_reward  \n",
    "    # Reward for entering the lose state\n",
    "    rewards['L'] = losing_reward\n",
    "    # Reward for entering an open location\n",
    "    rewards['O'] = intermediate_reward\n",
    "    global discount_factor\n",
    "    discount_factor = discount\n",
    "    reset_util(utility)\n",
    "    value_iteration(iterations)\n",
    "    \n",
    "# Create interactive sliders for \n",
    "# the rward values\n",
    "interact(interactive_iteration, discount=(0.0, 1.0, 0.05),\n",
    "                                winning_reward=(0.0, 1.0, 0.05),\n",
    "                                losing_reward=(-1.0, 0.0, 0.05),\n",
    "                                intermediate_reward=(-1.0, 1.0, 0.05),\n",
    "                                iterations=(0,1000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the transition probability can significantly change the policies.  In this case, we have a probability of the chosen action succeeding or moving in a perpendicular direction.  By decreasing the probability of success, we increase the uncertainty in the world, and by increasing the probability, the world's uncertainty decreases.  Observe how varying the level of uncertainty changes the policy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will reset the utility values\n",
    "# and then perform the value iteration \n",
    "# approach with the parameters\n",
    "def interactive_iteration(success=1.0,\n",
    "                          discount=0.8,\n",
    "                          winning_reward=1.0,\n",
    "                          losing_reward=-1.0,\n",
    "                          intermediate_reward=0.0,\n",
    "                          iterations=1000):\n",
    "    # Reward for entering the win state\n",
    "    rewards['W'] = winning_reward  \n",
    "    # Reward for entering the lose state\n",
    "    rewards['L'] = losing_reward\n",
    "    # Reward for entering an open location\n",
    "    rewards['O'] = intermediate_reward\n",
    "    global discount_factor\n",
    "    discount_factor = discount\n",
    "    global prob_action_success\n",
    "    prob_action_success = success\n",
    "    reset_util(utility)\n",
    "    value_iteration(iterations)\n",
    "    \n",
    "# Create interactive sliders for \n",
    "# the rward values\n",
    "interact(interactive_iteration, success=(0.0, 1.0, 0.05),\n",
    "                                discount=(0.0, 1.0, 0.05),\n",
    "                                winning_reward=(0.0, 1.0, 0.05),\n",
    "                                losing_reward=(-1.0, 0.0, 0.05),\n",
    "                                intermediate_reward=(-1.0, 1.0, 0.05),\n",
    "                                iterations=(0,1000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, MDP models are not known either because the underlying environment is poorly characterized or because the dynamics of the environment are too complex to be captured analytically. To address such learning scenarios with an unknown environment, the agent must dynamically interact with the environment using a policy, $\\pi$ in a sequence of discrete time steps, $t\\in\\mathbb{N}$. At each time step $t$, the learning agent observes the environment's state, $s_{(t)}\\in\\mathcal{S}$ and selects an action $a_{(t)}\\in\\mathcal{A}$ based upon $\\pi(s_{t})$. The next time step, the learning agent receives a numerical reward $r_{(t+1)}$ depending on its new state $s_{(t+1)}$.\n",
    "\n",
    "The goal of RL in this domain is to solve the planning problem of an MDP solely through interactions with the environment and without knowledge of the model that characterizes the MDP. Compared to the planning problem of a fully-known MDP, an RL agent must overcome the following challenges:\n",
    "\n",
    "- The learning agent does not have access to all states at once. Thus, samples are gathered as part of a trajectory.\n",
    "- The learning agent cannot directly access the model parameters, i.e., $\\mathbf{P}$ and $\\mathbf{r}$.\n",
    "- The per-time-step complexity is limited by the time between interactions with the environment.\n",
    "\n",
    "Due to their reliance on samples, such methods are sensitive to the exploration methods used. The exploration method determines how the learning agent seeks out new samples from the environment while still attempting to maximize reward.\n",
    "\n",
    "These methods that learned from a sequence of events are collectively known as Temporal-Difference (TD) Learning, since they rely on reward differences in time to learn policies. At time $t$, TD methods update the value function estimates using the observed reward $r_{(t)}$ at time $t$, and the estimate of $V(s_{(t+1)})$, where $s_{(t+1)}$ denotes the state observed by the system at $t+1$. The update followed by the simplest TD method, known as TD(0), is\n",
    "\n",
    "$$\n",
    "V(s_{(t)})\\leftarrow V(s_{(t)})+\\alpha\\left[r_{(t)}+\\gamma V(s_{(t+1)})-V(s_{(t)})\\right].\n",
    "$$\n",
    "\n",
    "Note that the TD method bases its updates in part on an existing estimate of the state value function $V$.  Additionally, observe the parameter $\\alpha$ that represents the <i>learning rate</i>, that is, how much of an impact newly observed information has versus prior estimates. TD methods do not require a model of the MDP, are naturally implemented in an on-line and fully incremental fashion, and can learn from each transition regardless of what subsequent actions are taken.\n",
    "\n",
    "Let us now consider how TD methods can be used in RL. The main idea is to proceed according to the notion of a generalized policy iteration (GPI). In GPI one maintains both an approximate policy and an approximate state value function. The state value function is repeatedly altered to more closely approximate the state value function for the current policy, and the policy is repeatedly improved with respect to the current $V$. If a model is not available, then it is particularly useful to estimate a state-action value function $Q$ rather than $V$. With a model, $V$ alone is sufficient to determine a policy by simply looking ahead one step and choosing whichever action leads to the best combination of reward and next state. Without a model, however, $V$ alone is not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. \n",
    "\n",
    "The policy evaluation problem for a state-action value function is to estimate $Q^{\\pi}(s,a)$, that is, the expected return when starting in state $s$, taking action $a$, and thereafter following policy $\\pi$. Policy improvement is done by using a greedy policy with respect to the current value function. In this case, one has a state-action value function $Q$, and therefore no model is needed to construct the greedy policy. For any $Q$, the corresponding greedy policy is the one that, for each $s\\in\\mathcal{S}$, deterministically chooses an action with maximal $Q$-value as\n",
    "\n",
    "$$\n",
    "\\pi(s)=\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}Q(s,a).\n",
    "$$\n",
    "\n",
    "Another policy option that strikes a balance between exploration and exploitation is to use an $\\varepsilon$-greedy policy, which chooses random actions every time with a small probability $\\varepsilon\\in[0,1]$, and acts greedily otherwise as\n",
    "\n",
    "$$\n",
    "\\pi^{\\varepsilon}(s)=\\left\\{\\begin{array}{ll}\n",
    "\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}Q(s,a)&\\textrm{with probability 1-$\\varepsilon$}\\\\\n",
    "\\textrm{Choose $a\\in\\mathcal{A}$ uniformly at random}&\\textrm{with probability $\\varepsilon$}\n",
    "\\end{array}\\right..\n",
    "$$\n",
    "\n",
    "Next, we will explore two important instatiations of TD(0) learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3a. Q-learning: Off-policy TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In off-policy learning, the policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the estimation policy. An advantage of this separation is that the estimation policy may be simple to evaluate (e.g., a greedy policy), while the behavior policy can continue to sample all possible actions.\n",
    "\n",
    "$Q$-learning uses an $\\varepsilon$-greedy policy as its behavior policy, which guarantees that all actions are eventually sampled, and a greedy policy for estimating $Q^*$. $Q$-learning updates the $Q$-values as\n",
    "$$\n",
    "Q(s_{(t)},a_{(t)})\\leftarrow Q(s_{(t)},a_{(t)})+\\alpha\\left[r_{(t)}+\\gamma \\max_{a\\in\\mathcal{A}}Q(s_{(t+1)},a)-Q(s_{(t)},a_{(t)})\\right].\n",
    "$$\n",
    "\n",
    "To demonstrate, we will use the prior grid world domain.  Below are the necessary additions to perform $Q$-learning are implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.1 Interacting with the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because such approached learn by interacting with the world, first we create the means of tracking current state and performing actions to transition from one state to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Starting state\n",
    "# Represents the row and column\n",
    "# the agent starts at in the grid world\n",
    "start_x = 2\n",
    "start_y = 0\n",
    "\n",
    "# Current state\n",
    "# Represents the row and column\n",
    "# the agent is currently at in the grid world\n",
    "# Initialized to the start state\n",
    "cur_x = start_x\n",
    "cur_y = start_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that performs an action\n",
    "# in the grid world and then returns\n",
    "# the new state\n",
    "# Available actions are (U)p, (D)own, (L)eft, and (R)ight\n",
    "def perform_action(xx, yy, action):\n",
    "    \n",
    "    # If in any of these states, cannot move from them (Blocked, Win, Lost)\n",
    "    if(world[xx][yy] == 'B' or world[xx][yy] == 'W' or world[xx][yy] == 'L'):\n",
    "        # Reamin at current location\n",
    "        return xx,yy\n",
    "    \n",
    "    # Determine if move does not succeed with given probability\n",
    "    if(random.uniform(0, 1) > prob_action_success):\n",
    "        \n",
    "        # If original action is up or down, \n",
    "        # change the action to left on failure\n",
    "        if(action == 'U' or action == 'D'):\n",
    "            action = 'L'\n",
    "        \n",
    "        # Else original action is left or right\n",
    "        # change the action to up\n",
    "        else:\n",
    "            action = 'U'\n",
    "        \n",
    "        # Flip the direction of the failure\n",
    "        # with equal probability (e.g. either\n",
    "        # fail left/right, or up/down)\n",
    "        if(random.uniform(0, 1) < 0.5):\n",
    "            # Reverse directions\n",
    "            if(action == 'U'):\n",
    "                action = 'D'\n",
    "            else:\n",
    "                action = 'R'\n",
    "    \n",
    "    # Set the new state values\n",
    "    # to the current state values\n",
    "    new_x = xx\n",
    "    new_y = yy\n",
    "    \n",
    "    # Perform the movement by\n",
    "    # adding/substracting from\n",
    "    # appropriate state variables\n",
    "    if(action == 'U'):\n",
    "        new_x = new_x - 1\n",
    "    elif(action == 'D'):\n",
    "        new_x = new_x + 1\n",
    "    elif(action == 'L'):\n",
    "        new_y = new_y - 1\n",
    "    elif(action == 'R'):\n",
    "        new_y = new_y + 1\n",
    "    \n",
    "    # If the new state is out of bounds, \n",
    "    # or trying to move to a blocked location\n",
    "    # return the original state\n",
    "    if(new_y < 0 or new_y > 3 or new_x < 0 or new_x > 2):\n",
    "        return xx,yy\n",
    "    if(world[new_x][new_y] == 'B'):\n",
    "        return xx,yy\n",
    "    \n",
    "    # Return the new state\n",
    "    return new_x,new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.2 The Q-Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foundation of Q-Learning is the Q-table, which defines the reward for a given state and action: $Q(s,a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Q-table that contains the\n",
    "# approximation of the state-action \n",
    "# value function\n",
    "q_values = {}\n",
    "\n",
    "# Each action maps to a matrix of values\n",
    "# that represent the value of taking\n",
    "# that action in a particular state\n",
    "# Thus, q_values['U'][1][3] returns the\n",
    "# approximate reward for moving Up\n",
    "# when in the 1st row, 3rd column\n",
    "q_values['U'] = [[0, 0, 0, 0],\n",
    "                [0, 0, 0, 0],\n",
    "                [0, 0, 0, 0]]\n",
    "q_values['D'] = [[0, 0, 0, 0],\n",
    "                [0, 0, 0, 0],\n",
    "                [0, 0, 0, 0]]\n",
    "q_values['L'] = [[0, 0, 0, 0],\n",
    "                [0, 0, 0, 0],\n",
    "                [0, 0, 0, 0]]\n",
    "q_values['R'] = [[0, 0, 0, 0],\n",
    "                [0, 0, 0, 0],\n",
    "                [0, 0, 0, 0]]\n",
    "\n",
    "# Function to initialize the q-table\n",
    "# entries to a particular value\n",
    "def init_qtable(init_value):\n",
    "    for xx in range(3):\n",
    "        for yy in range(4):\n",
    "            q_values['U'][xx][yy] = init_value\n",
    "            q_values['D'][xx][yy] = init_value\n",
    "            q_values['L'][xx][yy] = init_value\n",
    "            q_values['R'][xx][yy] = init_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.3 Choosing Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with the environment, Q-Learning selects an action based upon the values in the Q-Table and a parameter for randomness, epsilon ($\\epsilon$).  This approach is known as the $\\epsilon$-greedy approach.\n",
    "\n",
    "$$\n",
    "\\pi^{\\varepsilon}(s)=\\left\\{\\begin{array}{ll}\n",
    "\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}Q(s,a)&\\textrm{with probability 1-$\\varepsilon$}\\\\\n",
    "\\textrm{Choose $a\\in\\mathcal{A}$ uniformly at random}&\\textrm{with probability $\\varepsilon$}\n",
    "\\end{array}\\right..\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rate of selecting action at random, \n",
    "# rather than greedy selection\n",
    "epsilon = 0.1\n",
    "\n",
    "# Chooses an action through the epsilon greedy approach\n",
    "def choose_action(xx, yy, q_table):\n",
    "    \n",
    "    # If we choose at random\n",
    "    if(random.uniform(0,1) < epsilon):\n",
    "        # Select Up, Down, Left, or Right\n",
    "        # in a uniform random manner\n",
    "        selection = random.uniform(0,1)\n",
    "        if(selection < 0.25):\n",
    "           return 'U'\n",
    "        elif(selection < 0.5):\n",
    "           return 'R'\n",
    "        elif(selection < 0.75):\n",
    "           return 'D'\n",
    "        else:\n",
    "            return 'L'\n",
    "        \n",
    "    # Else select the best action possible (greedy)\n",
    "    return best_action(xx, yy, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to select the best action from the Q-table ($\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}Q(s,a)$) is implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selects the best action \n",
    "# at the given state through the given q-table\n",
    "def best_action(xx, yy, q_table):\n",
    "    best = 'U'\n",
    "    if(q_table[best][xx][yy] < q_table['R'][xx][yy]):\n",
    "        best = 'R'\n",
    "    if(q_table[best][xx][yy] < q_table['D'][xx][yy]):\n",
    "        best = 'D'\n",
    "    if(q_table[best][xx][yy] < q_table['L'][xx][yy]):\n",
    "        best = 'L'\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.4 Updating the Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-values are updated according to the equation:\n",
    "$$\n",
    "Q(s_{(t)},a_{(t)})\\leftarrow Q(s_{(t)},a_{(t)})+\\alpha\\left[r_{(t)}+\\gamma \\max_{a\\in\\mathcal{A}}Q(s_{(t+1)},a)-Q(s_{(t)},a_{(t)})\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate of (alpha) Q-Learning\n",
    "learning_rate = 0.9\n",
    "# Discount factor (gamma) applied to future rewards\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Updates the Q-table given the current state, action taken,\n",
    "# and the next state according the the learning rate\n",
    "# and discoutn factor\n",
    "def update_q_table(q_table, xx1, yy1, action, xx2, yy2):    \n",
    "    \n",
    "    # If it is a final state\n",
    "    if(world[xx1][yy1] != 'O'):\n",
    "        # The q-value is simply the reward\n",
    "        q_table[action][xx1][yy1] = rewards[world[xx1][yy1]]\n",
    "        return\n",
    "    \n",
    "    # Find the optimal action of the next state\n",
    "    best = best_action(xx2, yy2, q_table)\n",
    "    \n",
    "    # Find the update value for the Q value\n",
    "    update = learning_rate * (rewards[world[xx1][yy1]] + discount_factor * q_table[best][xx2][yy2] - q_table[action][xx1][yy1])\n",
    "    \n",
    "    # Update the Q value for the action at the original state\n",
    "    q_table[action][xx1][yy1] = q_table[action][xx1][yy1] + update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.5 Deriving Policy from Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy is derived from the Q-values through:\n",
    "$$\n",
    "\\pi(s)=\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}Q(s,a).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updates the optimal policy with the optimal actions given the Q-values\n",
    "def update_policy(policy, q_values):\n",
    "    for xx in range(3):\n",
    "        for yy in range(4):\n",
    "            policy[xx][yy] = 'N'\n",
    "            if(world[xx][yy] == 'O'):\n",
    "                policy[xx][yy] = best_action(xx,yy,q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.5 Assembling the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function performs q-learning\n",
    "# for a given number of iterations\n",
    "def q_learn(iterations):\n",
    "  \n",
    "    cur_x = start_x\n",
    "    cur_y = start_y\n",
    "    \n",
    "    # For the number of iterations\n",
    "    for ii in range(iterations): \n",
    "        \n",
    "        # Choose an action (epsilon greedy)\n",
    "        action = choose_action(cur_x, cur_y, q_values)\n",
    "        \n",
    "        # Perform the action and move to the new state\n",
    "        new_x, new_y = perform_action(cur_x, cur_y, action)\n",
    "        \n",
    "        # Update the Q-table\n",
    "        update_q_table(q_values, cur_x, cur_y, action, new_x, new_y)\n",
    "        \n",
    "        # If in the end state, reset to the starting state\n",
    "        if(cur_x == new_x and cur_y == new_y and world[cur_x][cur_y] != 'O'):\n",
    "            new_x = start_x\n",
    "            new_y = start_y\n",
    "            \n",
    "            \n",
    "        # Set the current state to the new state\n",
    "        cur_x,cur_y = new_x,new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3a.6 Playing with Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two new parameters introduced into the Q-Learning paradigm: The learning rate, $\\alpha$, and the randomness parameter, $\\epsilon$. Change epsilon to accelerate the rate of exploration and the learning rate to change how quickly the Q values update with new information.  Other paramters from the value iteration approach remain and can have significant effect on Q-Learning.  Explore these interactions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will reset the utility values\n",
    "# and then perform the value iteration \n",
    "# approach with the parameters\n",
    "def interactive_iteration(learning = 0.25,\n",
    "                          randomness = 0.1,\n",
    "                          success=0.85,\n",
    "                          discount=0.9,\n",
    "                          winning_reward=1.0,\n",
    "                          losing_reward=-1.0,\n",
    "                          intermediate_reward=0.0,\n",
    "                          iterations=1000):\n",
    "    global learning_rate\n",
    "    learning_rate = learning\n",
    "    global epsilon\n",
    "    epsilon = randomness\n",
    "    # Reward for entering the win state\n",
    "    rewards['W'] = winning_reward  \n",
    "    # Reward for entering the lose state\n",
    "    rewards['L'] = losing_reward\n",
    "    # Reward for entering an open location\n",
    "    rewards['O'] = intermediate_reward\n",
    "    global discount_factor\n",
    "    discount_factor = discount\n",
    "    global prob_action_success\n",
    "    prob_action_success = success\n",
    "    \n",
    "    # Initializes the Q-table to be all 1's\n",
    "    init_qtable(1.0)\n",
    "\n",
    "    # Performs Q-Learning for iterations\n",
    "    q_learn(iterations)\n",
    "\n",
    "    # Updates the policy based on the Q-table\n",
    "    update_policy(policy, q_values)\n",
    "\n",
    "    # Print the Q-values for each action\n",
    "    print 'Up'\n",
    "    for xx in range(3):\n",
    "        print q_values['U'][xx]\n",
    "    print\n",
    "    print 'Down'\n",
    "    for xx in range(3):\n",
    "        print q_values['D'][xx]\n",
    "    print\n",
    "    print 'Left'\n",
    "    for xx in range(3):\n",
    "        print q_values['L'][xx]\n",
    "    print\n",
    "    print 'Right'\n",
    "    for xx in range(3):\n",
    "        print q_values['R'][xx]\n",
    "    print\n",
    "\n",
    "    # Print out the policy\n",
    "    print 'Policy'\n",
    "    for xx in range(3):\n",
    "        print policy[xx]   \n",
    "    \n",
    "# Create interactive sliders for \n",
    "# the rward values\n",
    "interact(interactive_iteration, learning=(0.0, 1.0, 0.05),\n",
    "                                randomness=(0.0, 1.0, 0.05),\n",
    "                                success=(0.0, 1.0, 0.05),\n",
    "                                discount=(0.0, 1.0, 0.05),\n",
    "                                winning_reward=(0.0, 1.0, 0.05),\n",
    "                                losing_reward=(-1.0, 0.0, 0.05),\n",
    "                                intermediate_reward=(-1.0, 1.0, 0.05),\n",
    "                                iterations=(0,10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3b. SARSA: On-policy TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, on-policy methods attempt to evaluate or improve the policy that is used to make decisions. In on-policy methods the policy is generally soft, meaning that there is a non-zero probability to select every feasible action while in state $s$. There are many possible variations on on-policy methods. Here, we focus on $\\varepsilon$-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability $\\varepsilon/|\\mathcal{A}|$, with $\\varepsilon>0$, they instead choose an action at random. \n",
    "\n",
    "In particular, we examine an algorithm called State, Action, Reward, State, Action (SARSA). An agent using SARSA interacts with the environment and updates the policy based on actions taken. The $Q$-value for a state-action pair is sequentially updated by an error quantity, adjusted by the learning rate $\\alpha>0$ which determines to what extent newly acquired information overrides old information. Note that SARSA learns the $Q$-values associated with taking the policy it follows. SARSA uses an $\\varepsilon$-greedy policy and updates the $Q$-values as\n",
    "\n",
    "$$\n",
    "Q(s_{(t)},a_{(t)})\\leftarrow Q(s_{(t)},a_{(t)})+\\alpha\\left[r_{(t)}+\\gamma Q(s_{(t+1)},a_{(t+1)})-Q(s_{(t)},a_{(t)})\\right].\n",
    "$$\n",
    "\n",
    "where $a_{(t)}$ denotes the action taken at time $t$. This rule uses every element of the quintuple of events, $(s_{(t)},a_{(t)},r_{(t)},s_{(t+1)},a_{(t+1)})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name SARSA for the algorithm.  Below, we define the methods required for SARSA learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Updates the Q-table given the current state, action taken, and the next state\n",
    "def update_q_table_sarsa(q_table, xx1, yy1, action1, xx2, yy2, action2):    \n",
    "    \n",
    "    # If in a final state\n",
    "    if(world[xx1][yy1] != 'O'):\n",
    "        # Q-value is the reward\n",
    "        q_table[action1][xx1][yy1] = rewards[world[xx1][yy1]]\n",
    "        return\n",
    "    \n",
    "    # Find the update value for the Q value\n",
    "    update = learning_rate * (rewards[world[xx1][yy1]] + discount_factor * q_table[action2][xx2][yy2] - q_table[action1][xx1][yy1])\n",
    "    \n",
    "    # Update the Q value for the action at the original state\n",
    "    q_table[action1][xx1][yy1] = q_table[action1][xx1][yy1] + update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it is a minor change from $Q$-learning to SARSA.  Instead of assuming the next action taken is the optimal, The Q-values are updated according the next action that was chosen.  All the same parameters from before apply, investigate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function performs SARSA\n",
    "# for a given number of iterations\n",
    "def sarsa(iterations):\n",
    "  \n",
    "    cur_x = start_x\n",
    "    cur_y = start_y\n",
    "    prev_x = start_x\n",
    "    prev_y = start_y\n",
    "    prev_action = -1\n",
    "        \n",
    "    # For the number of iterations\n",
    "    for ii in range(iterations): \n",
    "        \n",
    "        # Choose an action (epsilon greedy)\n",
    "        next_action = choose_action(cur_x, cur_y, q_values)\n",
    "        \n",
    "        if(prev_action != -1):\n",
    "            # Update the Q-table\n",
    "            update_q_table_sarsa(q_values, prev_x, prev_y, prev_action, cur_x, cur_y, next_action)\n",
    "      \n",
    "        \n",
    "        # Perform the action and move to the new state\n",
    "        new_x, new_y = perform_action(cur_x, cur_y, next_action)\n",
    "        \n",
    "        # If in the end state, reset to the starting state\n",
    "        if(cur_x == new_x and cur_y == new_y and world[cur_x][cur_y] != 'O'):\n",
    "            new_x = start_x\n",
    "            new_y = start_y\n",
    "            prev_action = -1\n",
    "        \n",
    "        # Set the current state to the new state\n",
    "        prev_action = next_action\n",
    "        prev_x,prev_y = cur_x,cur_y\n",
    "        cur_x,cur_y = new_x,new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will reset the utility values\n",
    "# and then perform the value iteration \n",
    "# approach with the parameters\n",
    "def interactive_iteration(learning = 0.2,\n",
    "                          randomness = 0.1,\n",
    "                          success=0.85,\n",
    "                          discount=0.8,\n",
    "                          winning_reward=1.0,\n",
    "                          losing_reward=-1.0,\n",
    "                          intermediate_reward=0.0,\n",
    "                          iterations=1000):\n",
    "    global learning_rate\n",
    "    learning_rate = learning\n",
    "    global epsilon\n",
    "    epsilon = randomness\n",
    "    # Reward for entering the win state\n",
    "    rewards['W'] = winning_reward  \n",
    "    # Reward for entering the lose state\n",
    "    rewards['L'] = losing_reward\n",
    "    # Reward for entering an open location\n",
    "    rewards['O'] = intermediate_reward\n",
    "    global discount_factor\n",
    "    discount_factor = discount\n",
    "    global prob_action_success\n",
    "    prob_action_success = success\n",
    "    \n",
    "    # Initializes the Q-table to be all 1's\n",
    "    init_qtable(1.0)\n",
    "\n",
    "    # Performs Q-Learning for iterations\n",
    "    sarsa(iterations)\n",
    "\n",
    "    # Updates the policy based on the Q-table\n",
    "    update_policy(policy, q_values)\n",
    "\n",
    "    # Print the Q-values for each action\n",
    "    print 'Up'\n",
    "    for xx in range(3):\n",
    "        print q_values['U'][xx]\n",
    "    print\n",
    "    print 'Down'\n",
    "    for xx in range(3):\n",
    "        print q_values['D'][xx]\n",
    "    print\n",
    "    print 'Left'\n",
    "    for xx in range(3):\n",
    "        print q_values['L'][xx]\n",
    "    print\n",
    "    print 'Right'\n",
    "    for xx in range(3):\n",
    "        print q_values['R'][xx]\n",
    "    print\n",
    "\n",
    "    # Print out the policy\n",
    "    print 'Policy'\n",
    "    for xx in range(3):\n",
    "        print policy[xx]   \n",
    "    \n",
    "# Create interactive sliders for \n",
    "# the rward values\n",
    "interact(interactive_iteration, learning=(0.0, 1.0, 0.05),\n",
    "                                randomness=(0.0, 1.0, 0.05),\n",
    "                                success=(0.0, 1.0, 0.05),\n",
    "                                discount=(0.0, 1.0, 0.05),\n",
    "                                winning_reward=(0.0, 1.0, 0.05),\n",
    "                                losing_reward=(-1.0, 0.0, 0.05),\n",
    "                                intermediate_reward=(-1.0, 1.0, 0.05),\n",
    "                                iterations=(0,10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. TD-Learning Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends our introduction to temporal difference learning.  We completed an overview of a basics of Markov Decision Process, vale-function learning and demonstrated the key components of Temporal Difference Learning algorithms ($Q$-Learning and SARSA).  As a reminder, the key parts to such approaches include:\n",
    "\n",
    "<ul>\n",
    "<li><b>Environment</b>:        World with which a learning agent interacts</li>\n",
    "       <li><b>State ($s$)</b>:        Observations that define the environment for the agent</li>\n",
    "       <li><b>State-Space ($S$)</b>:  All possible values of the state</li>\n",
    "       <li><b>Action ($a$)</b>:       Decision by agent that interacts with the environment</li>\n",
    "       <li><b>Action-Space ($A$)</b>: All possible actions the agent may make</li>\n",
    "       <li><b>State Transition:</b>   Moving from one state to another state given an action</li>\n",
    "       <li><b>Policy ($\\pi$)</b>:     Mapping of states to actions ($\\pi(s)=a$)</li>\n",
    "       <li><b>Reward ($r$)</b>:       Benefit received from a particular state</li>\n",
    "</ul>\n",
    "\n",
    "There is a vast amount of literature out there if you wish to learn more, from basic text books like <i>Reinforcement Learning: an introduction</i> by Richard S. Sutton and Andrew G. Barto, to cutting edge international conferences, such as the International Conference on Machine Learning (ICML) and the Autonomous Agents and Multi-Agent Systems conference (AAMAS).  \n",
    "\n",
    "This has been a small taste of the rich theory and applications that exists in TD-Learning research.  Additions to address topics such as continuous and large state spaces include tile-coding and using fuction approximators to estimate rewards for continuous states.  One of the most important areas of current research is to eliminate or loosen the foundational assumptions that allow such methods to work, such as the state representation requiring the Markov property (see POMDPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evolutionary Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many ways, evolution can be considered the ultimate reinforcement learning algorithm.  All it cares about is the propagation of genes, that is, how well does a living being replicate and allow their genes to persist. Every living being (agent) takes actions in response to stimulus from their environment (state); however, we do not necessarily know how much any particular action contributes to the propagation of their genes (reward).  \n",
    "\n",
    "Evolutionary approaches have several advantages over other reinforcement learning approaches ([Schmidhuber, 2000](ftp://ftp.idsia.ch/pub/juergen/seal2000.pdf)). In particular, unlike value-function approaches, evolutionary approaches do not rely on Markov assumptions, such as full observability of the world.  For an idea of the breadth and depth of research considered part of evolutionary computation, please check out the Genetic and Evolutionary Computation Conferences (GECCOs), hosted by [ACM's Special Interest Group in Evolution (SIGEVO)](http://www.sigevo.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Overview and Key Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolutionary computation mainly began in the 1960's-70's in three separate strains of research.  The first, evolutionary programming focused on creating a system that would automate the process of programming software.  This automation was performed through an evolutionary process applied to finite state machines.  Second, evolutionary strategies focused on using evolution to optimize real-valued parameters for problem domains.  Finally, genetic algorithms examined directly evolving the binary representation of a solution to a problem in order to solve it.  Starting in the 1990's, these fields recognized they were highly related and began to unify. For a more complete history, see [Evolutionary Computation: a unified approach](http://mitpress.mit.edu/books/evolutionary-computation) by Kenneth De Jong.\n",
    "\n",
    "\n",
    "Despite the varied background of the beginnings of evolutionary computation, they all shared several characteristics:\n",
    "<ul>\n",
    "<li><b>Individuals (or, Genomes):</b> Candidate solution instances to the problem domain</li>\n",
    "<li><b>Populations:</b> Sets of individuals</li>\n",
    "<li><b>Fitness:</b> A measure of an individual's performance in the problem domain</li>\n",
    "<li><b>Reproduction:</b> Process by which existing individuals (<b>parents</b>) produce new individuals (<b>offspring, or children</b>)</li>\n",
    "<li><b>Selection:</b> A process by which either (1) parents are chosen to create offspring, or, (2) individuals are chosen to die and be replaced </li>\n",
    "</ul>\n",
    "\n",
    "And all follow a similar algorithmic outline:\n",
    "\n",
    "<ul>\n",
    "<li><b>BEGIN Evolution:</b>\n",
    "    <ul>\n",
    "    <li>Initialize population (Create set of random genomes and evaluate)</li>\n",
    "    <li><b>DO</b>:\n",
    "    <ul>\n",
    "        <li>Select parents from population</li>\n",
    "        <li>Reproduce parents to create offspring</li>\n",
    "        <li>Evaluate offspring</li>\n",
    "        <li>Select individuals to make next population from current population + offspring</li> \n",
    "    </ul>\n",
    "    </li>\n",
    "    <li><b>WHILE</b>: Stopping criteria not met</li>\n",
    "    </ul>\n",
    "<li><b>DONE</b></li>\n",
    "</ul>\n",
    "\n",
    "From this simple loop, a multitude of approaches can be implemented by varying the details of each step.  For evaluation, not only can the domain change, but alternatives such as co-evolution (individuals compete against each other) or real-time evolution (the population is continuously evaluated, with offspring added to the ongoing evaluation).  Reproduction can be applied through <i>Mutation</i>, where an existing genome is cloned and then has random changes made to it, or, <i>Recombination</i>, where two or more genomes have their genes jumbled together.  Mutation is also called <i>Asexual Reproduction</i> and Recombination can be called <i>Crossover</i> or <i>Sexual Reproduction</i>.  Finally, selection approaches can vary from uniform random selection (neutral) to truncation selection (high pressure).  \n",
    "\n",
    "All of these topics will be explored in the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Constructing an Evolutionary Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. The Genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these exercises, we will construct a basic evolutionary algorithm by exploring function optimization through the evolution of real-valued parameters.  To begin, we need to define the form our solution takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Genome class encapsulates information about a genome\n",
    "# including fitness and the genetic representation \n",
    "# of the solution plus useful utility functions\n",
    "class Genome: \n",
    "    # Initializer for new genome instance\n",
    "    def __init__(self, fitness, genes):\n",
    "        # Set the instance's fitness to the given fitness\n",
    "        self.fitness = fitness\n",
    "        # Set the instance's genese to the given genes\n",
    "        self.genes = genes \n",
    "    \n",
    "    # Creates a deep clone of the genome\n",
    "    def clone(self): \n",
    "        # Uses copy's deepcopy function to create a deep copy of this Genome instance\n",
    "        return copy.deepcopy(self) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the genome consists of a fitness value and a set of genes.  For the next few experiments, we will define the set of genes as a single value $x\\in\\mathbb{R}$.  Below, we define a function to generate a new, random genome with $x\\in[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates and returns a new random genome\n",
    "def new_genome():   \n",
    "    # A random genome for this domain is a real valued number (-1, 1)\n",
    "    return Genome(-10000, random.uniform(-1, 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Evaluating Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closely linked to the genes that represent the solution is how performance is evaluated.  This means of evaluation can also be called the domain.  In this example, we are trying to optimize the function $f(x)= 1.0 - x^2$, were $x$ is the single real-valued gene of the genome. Thus fitness can simply be described as the result of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluator that returns fitness of the genome\n",
    "def evaluate(genome): \n",
    "    # This fitness is equal to 1 - x^2\n",
    "    genome.fitness = 1.0 - genome.genes * genome.genes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. (un)Natural Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After specifying what a genome is and how it is evaluated, the next step becomes selecting among genomes based upon their fitness score.  In evolutionary algorithms this can be done at two different points (1) Selecting parents to produce offspring, and, (2) Selecting individuals that survive to the next generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we give each genome in the population an equal probability of being selected as a parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selects the given number of parents from the given population\n",
    "def select_parents(population, number): \n",
    "    \n",
    "    # Initialize an empty list of parents\n",
    "    parents = []  \n",
    "    \n",
    "    # Get the number of individuals in the given population\n",
    "    populationSize = len(population)\n",
    "    \n",
    "    # For the number of parents\n",
    "    for x in range(number): \n",
    "        \n",
    "        # Select a random genome from the population\n",
    "        # with uniform probability to be a parent\n",
    "        parents.append(population[random.randint(populationSize)]) \n",
    "    \n",
    "    # Return the parents list\n",
    "    return parents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting: The Next Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of selection is determining which genomes survive from one generation to the next.  In this instance, select a set number of genomes (the population size) from the combined pool of the existing population and newly created offspring through a process called <i>tournament selection</i>.  Tournament selection works by randomly selecting two genomes with uniform probability from the pool and selecting the genome with greater fitness to take a place in the next generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select a new population of individuals from the current and offspring\n",
    "def select_population(currentPopulation, offspring): \n",
    "    \n",
    "    # Get the size of the current population\n",
    "    popSize = len(currentPopulation)\n",
    "    \n",
    "    # Initialize an empty list for the new population\n",
    "    newPopulation = [] \n",
    "    \n",
    "    # Combine the current and offspring populations\n",
    "    currentPopulation.extend(offspring) \n",
    "    \n",
    "    # Get the combined size of current and offspring populations\n",
    "    combinedSize = len(currentPopulation)\n",
    "    \n",
    "    # Until the population for the next generation\n",
    "    # is the size of the current population\n",
    "    # keep selecting genomes\n",
    "    while(len(newPopulation) < popSize): \n",
    "        \n",
    "        # Select an index for the first candidate genome\n",
    "        candidateOneIdx = random.randint(combinedSize)  \n",
    "        # Select an index for the second candidate genome\n",
    "        candidateTwoIdx = random.randint(combinedSize)  \n",
    "        \n",
    "        # If the same index is selected\n",
    "        if(candidateOneIdx == candidateTwoIdx): \n",
    "            # Increment second index by one and make sure it is within bounds\n",
    "            candidateTwoIdx = (candidateTwoIdx + 1)%combinedSize\n",
    "            \n",
    "        # Set the selected genome to the first candidate\n",
    "        selected = currentPopulation[candidateOneIdx]  \n",
    "        \n",
    "        # If the second candidate's fitness is better\n",
    "        # set it to be selected\n",
    "        if(currentPopulation[candidateTwoIdx].fitness > selected.fitness): \n",
    "            selected = currentPopulation[candidateTwoIdx]\n",
    "            \n",
    "        # Add the selected genome to the new population\n",
    "        # and remove it from the pool of candidates\n",
    "        newPopulation.append(selected) \n",
    "        currentPopulation.remove(selected)\n",
    "        \n",
    "        # Decrement the pool count\n",
    "        combinedSize -= 1\n",
    "        \n",
    "    # Return the new population\n",
    "    return newPopulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4. Creating Offspring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect closely linked to the problem domain and genetic representation of the solution is how to create new genomes from the existing population.  Broadly speaking, there are two main approaches to generating new solutions, <i>mutation</i> (applied here), and, <i>crossover</i> (discussed later).  Mutation works by making a clone of a parent and then perturbing their genes through some process, often randomly.  Below are two functions: One that clones and mutates each of the parents, and, One that defines how the genes will be mutated.  In this case, the mutation will be a random perturbation of the real-valued gene in the range $[-0.01,0.01]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes in a set of parents and produces \n",
    "# an equivalent number of offspring\n",
    "def reproduce(parents):\n",
    "    \n",
    "    # Initialize an empty list of offspring\n",
    "    offspring = [] \n",
    "    \n",
    "    # For each parent, create a clone\n",
    "    # and mutate the clone\n",
    "    for parent in parents: \n",
    "        \n",
    "        # Clone the parent to produce a child\n",
    "        child = parent.clone() \n",
    "        \n",
    "        # Set its fitness to a very low value\n",
    "        child.fitness = -10000 \n",
    "        \n",
    "        # Mutate the clone child\n",
    "        mutate_genome(child) \n",
    "        \n",
    "        # Add the child to the offspring list\n",
    "        offspring.append(child) \n",
    "        \n",
    "    # Return the list of offspring\n",
    "    return offspring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutation takes a genome and alters \n",
    "# its genes to create a new solution instance\n",
    "def mutate_genome(genome): \n",
    "    \n",
    "    # This mutatation adds a value between \n",
    "    # -0.01 and 0.01 to the genome's genes\n",
    "    genome.genes += random.uniform(-0.01, 0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5. The Loop of Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to construct the evolutionary loop.  In this implementation, we are going to define a function that takes sizes for population and number of offspring and a maximum number of iterations to perform. Note: Each iteration of evolution is called a <i>generation</i>.  In this example, evolution will stop after the set number of generations.  During evolution, the loop will keep track of the best genome found, the average and best fitness at each generation, and genes of the best genome at each generation.  These values will be plotted once evolution has been completed. The function will then return the genome that achieved the best fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function will run a set number of iterations of evolution\n",
    "# with a given population size and number of offspring\n",
    "# to create each generation\n",
    "def evolution(populationSize, offspringSize, maxGenerations):\n",
    "    \n",
    "    # Current generation being executed\n",
    "    currentGeneration = 0 \n",
    "    \n",
    "    # Array to hold generational average fitness values\n",
    "    avgFitnessByGeneration = [] \n",
    "    # Array to hold generational champion fitness values\n",
    "    bestFitnessByGeneration = [] \n",
    "    # Array to hold generational champion values\n",
    "    championValuesByGeneration = [] \n",
    "    \n",
    "    # Initialize an empty population\n",
    "    population = []  \n",
    "    # Set the best genome to a null value\n",
    "    bestGenome = None \n",
    "    \n",
    "    # Initialize variable for calculating population average fitness to 0\n",
    "    avg = 0.0  \n",
    "    \n",
    "    # Create and evaluate an initial population\n",
    "    # of genomes\n",
    "    for x in range(populationSize):\n",
    "        \n",
    "        # Create new random genome and add it to the population\n",
    "        population.append(new_genome())\n",
    "        # Evaluate that new genome\n",
    "        evaluate(population[x]) \n",
    "        # Add its fitness score to the average\n",
    "        avg += population[x].fitness \n",
    "        \n",
    "        # If the best genome hasn't been set, or, this\n",
    "        # genome has better fitness\n",
    "        if(bestGenome is None or population[x].fitness > bestGenome.fitness):  \n",
    "            \n",
    "            # Set the bestGenome to this genome\n",
    "            bestGenome = population[x] \n",
    "    \n",
    "    # Add the metrics to their various arrays for tracking\n",
    "    avgFitnessByGeneration.append(avg/populationSize)\n",
    "    bestFitnessByGeneration.append(bestGenome.fitness)\n",
    "    championValuesByGeneration.append(bestGenome.genes)\n",
    "    \n",
    "    # Until the maxmium number of generations \n",
    "    # is performed\n",
    "    while(currentGeneration < maxGenerations): \n",
    "        \n",
    "        # Select a number of parents equal \n",
    "        # to the number of offspring to be created\n",
    "        parents = select_parents(population, offspringSize)\n",
    "        \n",
    "        # Create a number of offspring by \n",
    "        # reproducing the selected parents\n",
    "        offspring = reproduce(parents) \n",
    "        \n",
    "        # Evaluate each of the offspring\n",
    "        for child in offspring:\n",
    "            \n",
    "            evaluate(child) \n",
    "            \n",
    "            # Update the best genome is applicable\n",
    "            if(child.fitness > bestGenome.fitness): \n",
    "                bestGenome = child\n",
    "        \n",
    "        # Select a new population from the \n",
    "        # current population and offspring\n",
    "        population = select_population(population, offspring) \n",
    "        \n",
    "        # Initialize average to 0\n",
    "        avg = 0.0 \n",
    "        \n",
    "        # Sum the fitness of the population\n",
    "        for genome in population: \n",
    "            avg += genome.fitness \n",
    "   \n",
    "        # Add the metrics to the various arrays\n",
    "        avgFitnessByGeneration.append(avg/populationSize)\n",
    "        bestFitnessByGeneration.append(bestGenome.fitness)\n",
    "        championValuesByGeneration.append(bestGenome.genes)\n",
    "        \n",
    "        # Increment current generation by one\n",
    "        currentGeneration += 1 \n",
    "    \n",
    "    # Evolution Loop Done\n",
    "    \n",
    "    # Plot the best fitness, average fitness, and the champion gene values by generation\n",
    "    labels = range(len(bestFitnessByGeneration))\n",
    "    subplot(2,1,1)\n",
    "    plot(labels, bestFitnessByGeneration, labels, avgFitnessByGeneration)\n",
    "    ylabel(\"Fitness\")\n",
    "    legend([\"Best\", \"Avg.\"], loc='lower center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "    subplot(2,1,2)\n",
    "    plot(labels, championValuesByGeneration)\n",
    "    ylabel(\"Gene Values\")\n",
    "    \n",
    "    # Return the best genome\n",
    "    return bestGenome "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6. Test Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our evolutionary algorithm, let us test it with a population size of one, that produces one offspring, and runs for 1000 generations.  At the end, the genes and the fitness of the best individual will be output, along with a graph of performance over the generations. Because we are optimizing for $f(x) = 1 - x^2$, the optimal value will  be $x = 0$, producing a fitness of $1.0$. Note: Because the population and offspring are both size one, the average fitness is equal to the best fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run evolution with a population of size 1\n",
    "# and creting 1 offspring per generation\n",
    "# for 1000 generations\n",
    "winner = evolution(1, 1, 1000)\n",
    "\n",
    "# Print out the best genome's genese and fitness\n",
    "print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. The Evaluation (Fitness) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that the algorithm is complete, we will explore different aspects of the evolutionary approach.  One of the strengths of the evolutionary approach is its minimial assumptions about the domain and the simple need to receive feedback from the evaluation function. This advantage means it is possible to alter the evaluation function without changing any other part of the algorithm. Try out your own fitness functions by changing the evaluate function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to evaluate a genome's fitness\n",
    "def evaluate(genome):\n",
    "    genome.fitness = 1.0 / (1.0 + abs(math.cos(genome.genes) - math.sin(genome.genes)))\n",
    "\n",
    "winner = evolution(1, 1, 1000)\n",
    "print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Growing the Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of evolution can heavily depend on chosen parameters.  These parameters control the balance between exploration (finding new solutions), exploitation (optimizing existing solution), and how we traverse the fitness landscape.  The first paremeters we will look at are the <i>population</i> parameters.  These parameters include <b>$\\mu$</b> (the population size) and <b>$\\lambda$</b> (the offspring size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population Size ($\\mu$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first parameter $\\mu$, this represents the number of individuals that will be maintained in the population.  One way of looking at it is $\\mu$ is the number of parallel search paths we are actively searching through, that is, each individual represents a particular search area in the landscape.  In this way, $\\mu$ controls the degree of exploration in the algorithm.  The higher the value, the more areas that are being searched.  However, the trade-off is that the greater the number of areas being searched, the less each area will have time be optimized.  Examine the results below as $\\mu$ varies.  Take particular note of how quickly the average fitness converges on the best fitness, which is a measure of how exploitative the algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will perform evolution \n",
    "# with the given parameters\n",
    "def interactive_iteration(generations=1000,\n",
    "                          mu=1):\n",
    "    if(mu == 0):\n",
    "        mu = 1\n",
    "    winner = evolution(mu, 1, generations)\n",
    "\n",
    "    print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offspring Size ($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second parameter, $\\lambda$, similarly balances exploration versus exploitation.  The greater the number of offspring generated, the more search is conducted around the existing population points.  For example, with $\\mu = 1$ and $\\lambda = 100$, 100 different points around the existing indivudal are explored.  The exploitation tradeoff with $\\lambda$ is that the increased number of offspring correspond the increased computational cost in the number of evaluations per generation, thus for the same computational cost you can perform fewer generations (less optimization). Examine the results below as $\\lambda$ varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will perform evolution \n",
    "# with the given parameters\n",
    "def interactive_iteration(generations=1000,\n",
    "                          mu=1,\n",
    "                          lamb=1):\n",
    "    if(mu == 0):\n",
    "        mu = 1\n",
    "    if(lamb == 0):\n",
    "        lamb = 1\n",
    "    winner = evolution(mu, lamb, generations)\n",
    "\n",
    "    print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Explore the interaction between $\\mu$ and $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try out different combinations of $\\mu$ and $\\lambda$ on your own, with different fitness functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(genome):\n",
    "    genome.fitness = math.cos(genome.genes + 3) / (1.0 + abs(genome.genes))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we examined the effect that population and offspring size can have in evolution.  In the next section, we will examine the importance of selection in evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. Selecting the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Selection</i> plays a significant role in how well the evolutionary algorithm performs and there are a number of different approaches to selection.  Additionally, there are two points where selections comes into effect: Selecting parents to produce children, and, Selecting members of the current population and offspring to become part of the next population.  By convention, selective pressure is often applied at only one of these points, leaving the other to uniform random (neutral) selection. In this section we will discuss different selection approaches and what selection <i>means</i> at the different points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Pool of Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us examine selecting for the next population.  Selecting for the next population has meaning in the sense of asking, of the given points in the space we are currently at, which points do we want to maintain in our seach.  Looking at our previously defined select_population function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select a new population of individuals from the current and offspring\n",
    "def select_population(currentPopulation, offspring): \n",
    "    \n",
    "    # Get the size of the current population\n",
    "    popSize = len(currentPopulation)\n",
    "    \n",
    "    # Initialize an empty list for the new population\n",
    "    newPopulation = [] \n",
    "    \n",
    "    # Combine the current and offspring populations\n",
    "    currentPopulation.extend(offspring) \n",
    "    \n",
    "    # Get the combined size of current and offspring populations\n",
    "    combinedSize = len(currentPopulation)\n",
    "    \n",
    "    # Until the population for the next generation\n",
    "    # is the size of the current population\n",
    "    # keep selecting genomes\n",
    "    while(len(newPopulation) < popSize): \n",
    "        \n",
    "        # Select an index for the first candidate genome\n",
    "        candidateOneIdx = random.randint(combinedSize)  \n",
    "        # Select an index for the second candidate genome\n",
    "        candidateTwoIdx = random.randint(combinedSize)  \n",
    "        \n",
    "        # If the same index is selected\n",
    "        if(candidateOneIdx == candidateTwoIdx): \n",
    "            # Increment second index by one and make sure it is within bounds\n",
    "            candidateTwoIdx = (candidateTwoIdx + 1)%combinedSize\n",
    "            \n",
    "        # Set the selected genome to the first candidate\n",
    "        selected = currentPopulation[candidateOneIdx]  \n",
    "        \n",
    "        # If the second candidate's fitness is better\n",
    "        # set it to be selected\n",
    "        if(currentPopulation[candidateTwoIdx].fitness > selected.fitness): \n",
    "            selected = currentPopulation[candidateTwoIdx]\n",
    "            \n",
    "        # Add the selected genome to the new population\n",
    "        # and remove it from the pool of candidates\n",
    "        newPopulation.append(selected) \n",
    "        currentPopulation.remove(selected)\n",
    "        \n",
    "        # Decrement the pool count\n",
    "        combinedSize -= 1\n",
    "        \n",
    "    # Return the new population\n",
    "    return newPopulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, there are two items that define this selection approach.  First, the current population and the offspring are combined into a single pool for selection.  This pooling means that offspring compete with parents for slots in the next population, referred to as ($\\mu + \\lambda$), or, inter-generational selection.  Alternatively, generational selection, or ($\\mu$,$\\lambda$), eliminates parents to make space for the offspring and then competition for slots only exists among the offspring.  As an exercise, try to modify the above function, such that ($\\mu$,$\\lambda$) selection is taking place.  Hint: There are two conditions to consider (1) $\\mu > \\lambda$, and, (2) $\\mu \\leq \\lambda$.  Under the first condition, members of the population must be eliminated until there is $\\lambda$ space, and the second condition means that the offspring must be reduced to $\\mu$ members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods of Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second important part of this function is <i>how</i> selection is performed.  In this case, we can see that two genomes are selected (candidateOneIdx and candidateTwoIdx) at random from the candidate pool.  The fitness of these two candidates are then compared and the fitter of the two are selected.  This selection process is called <i>Tournament Selection</i>.  In this case, it is a tournament of size two, noted as $k=2$.  The $k$ value can be take on any value less than or equal to the size of the candidate pool; however, the larger the $k$, the greater the selective pressure.  Tournament selection is often chosen because it is computationally easy and because different $k$ probabilistically approximate most other selection approaches.\n",
    "\n",
    "Other selections approaches include (listed here from weakest to strongest pressure):\n",
    "<ul>\n",
    "<li><b>Uniform</b>: Each candidate has equal probability of being selected (fitness is not compared)</li>\n",
    "<li><b>Fitness Proportional</b>: Each candidate is assigned a probability that is proportional to its fitness relative to the population</li>\n",
    "<li><b>Linear Ranking</b>: Each candidate is assigned a linearly decreasing probability relative to its ranking order in the population</li>\n",
    "<li><b>Non-linear Ranking</b>: Each candidate is assigned a non-linearly decreasing probability relative to its ranking order in the population</li>\n",
    "<li><b>Truncation</b>: The top $n$ candidates have probability $1/n$, the rest are discarded</li>\n",
    "</ul>\n",
    "\n",
    "Below, the select_population function has been modified to do a tournament of size 3.  As an exercise, attempt to implement a tournament selection of arbitrary size or one of the above non-tournament selection approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a new population of individuals from the current and offspring\n",
    "def select_population(currentPopulation, offspring): \n",
    "    \n",
    "    # Get the size of the current population\n",
    "    popSize = len(currentPopulation) \n",
    "    \n",
    "    # Initialize an empty list for the new population\n",
    "    newPopulation = [] \n",
    "    \n",
    "    # Combine the current and offspring populations\n",
    "    currentPopulation.extend(offspring) \n",
    "    \n",
    "    # Get the combined size of current and offspring populations\n",
    "    combinedSize = len(currentPopulation)\n",
    "    \n",
    "    # Until we have selected to the population size\n",
    "    while(len(newPopulation) < popSize): \n",
    "        \n",
    "        # Select indicies for the candidate genomes\n",
    "        candidateOneIdx = random.randint(combinedSize)  \n",
    "        candidateTwoIdx = random.randint(combinedSize)  \n",
    "        candidateThreeIdx = random.randint(combinedSize)\n",
    "        \n",
    "        # If all three indicies are the same\n",
    "        if(candidateOneIdx == candidateTwoIdx and candidateOneIdx == candidateThreeIdx):\n",
    "            \n",
    "            # Increment second index by one,\n",
    "            # make sure it is within bounds\n",
    "            # and increment the third index\n",
    "            # by two\n",
    "            candidateTwoIdx = (candidateTwoIdx + 1)%combinedSize \n",
    "            candidateThreeIdx = (candidateThreeIdx + 2)%combinedSize\n",
    "        \n",
    "        # If first and second index are the same\n",
    "        if(candidateOneIdx == candidateTwoIdx): \n",
    "            # Increment second index by one\n",
    "            candidateTwoIdx = (candidateTwoIdx + 1)%combinedSize\n",
    "            \n",
    "        # If first and third index are the same\n",
    "        if(candidateOneIdx == candidateThreeIdx): \n",
    "            # Increment third index by one\n",
    "            candidateThreeIdx = (candidateThreeIdx + 1)%combinedSize\n",
    "            \n",
    "        # If second and third index are the same\n",
    "        if(candidateTwoIdx == candidateThreeIdx): \n",
    "            # Increment third index by one\n",
    "            candidateThreeIdx = (candidateThreeIdx + 1)%combinedSize   \n",
    "        \n",
    "        # Set the selected genome to the first candidate\n",
    "        selected = currentPopulation[candidateOneIdx]  \n",
    "        \n",
    "        # If the second candidate's fitness is better\n",
    "        # Set selected to the second candidate\n",
    "        if(currentPopulation[candidateTwoIdx].fitness > selected.fitness): \n",
    "            selected = currentPopulation[candidateTwoIdx] \n",
    "        \n",
    "        # If the third candidate's fitness is better\n",
    "        # Set selected to the third candidate\n",
    "        if(currentPopulation[candidateThreeIdx].fitness > selected.fitness): \n",
    "            selected = currentPopulation[candidateThreeIdx]\n",
    "            \n",
    "        # Add the selected genome to the new population\n",
    "        newPopulation.append(selected) \n",
    "        # Remove the selected genome from the current population\n",
    "        currentPopulation.remove(selected)\n",
    "        # Decrement the size of the current population by one\n",
    "        combinedSize -= 1 \n",
    "        \n",
    "    # Return the new population    \n",
    "    return newPopulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, selection pressure can be applied through the selection of parents.  This point of selection has a subtly different meaning than selecting for the next population and indeed different selection <i>methods</i> have different meanings here.  When selecting parents, what the pressure is telling evolution is which points do we currently want to explore around.  Examine the previsouly defined select_parents function below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selects the given number of parents from the given population\n",
    "def select_parents(population, number): \n",
    "    \n",
    "    # Initialize an empty list of parents\n",
    "    parents = []  \n",
    "    \n",
    "    # Get the number of individuals in the given population\n",
    "    populationSize = len(population)\n",
    "    \n",
    "    # For the number of parents\n",
    "    for x in range(number): \n",
    "        \n",
    "        # Select a random genome from the population\n",
    "        # with uniform probability to be a parent\n",
    "        parents.append(population[random.randint(populationSize)]) \n",
    "    \n",
    "    # Return the parents list\n",
    "    return parents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that there is no selection pressure here, that is, the selection is uniform random.  Second, replacement is taking place, thus any genome may occur multiple times in the parent list thereby producing multiple children.  This replacement is where the subtly different meaning impacts the different selection methods.  For example, fitness proportional selection means the proportion of new search points around a genome is determined by its relative fitness, that is, the selection probability assigned to a genome represents its expected number of children. As an example, below we remove the pressure from select_population, and add it to select_parents by making population selection uniform random and the parent selection done through tournament selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selects the given number of parents from the given population\n",
    "def select_parents(population, number):\n",
    "    \n",
    "    # Initialize an empty list of parents\n",
    "    parents = []  \n",
    "    # Get the number of individuals in the given population\n",
    "    populationSize = len(population) \n",
    "    \n",
    "    # For the number of parents\n",
    "    for x in range(number): \n",
    "        \n",
    "        # Select indicies for the candidate parents\n",
    "        candidateOneIdx = random.randint(populationSize)  \n",
    "        candidateTwoIdx = random.randint(populationSize)  \n",
    "        \n",
    "        # Set selected to the first choice\n",
    "        selected = population[candidateOneIdx]\n",
    "        \n",
    "        # If second choice has better fitness\n",
    "        # Set selected to the second choice\n",
    "        if(selected.fitness < population[candidateTwoIdx].fitness): \n",
    "            selected = population[candidateTwoIdx]\n",
    "            \n",
    "        # Add the selected parent\n",
    "        parents.append(selected) \n",
    "        \n",
    "    # Return the parents list\n",
    "    return parents \n",
    "\n",
    "# Select a new population of individuals from the current and offspring\n",
    "def select_population(currentPopulation, offspring):\n",
    "    \n",
    "    # Get the size of the current population\n",
    "    popSize = len(currentPopulation)\n",
    "    # Initialize an empty list for the new population\n",
    "    newPopulation = [] \n",
    "    \n",
    "    # Combine the current and offspring populations\n",
    "    currentPopulation.extend(offspring) \n",
    "    # Get the combined size of current and offspring populations\n",
    "    combinedSize = len(currentPopulation) \n",
    "    \n",
    "    # Until we reach the correct population size\n",
    "    while(len(newPopulation) < popSize):\n",
    "        \n",
    "        # Select index for candidate\n",
    "        candidateOneIdx = random.randint(combinedSize)\n",
    "        \n",
    "        # Get the selected genome\n",
    "        selected = currentPopulation[candidateOneIdx]\n",
    "        \n",
    "        # Add the selected genome to the new population\n",
    "        newPopulation.append(selected) \n",
    "        \n",
    "        # Remove the selected genome from the current population\n",
    "        currentPopulation.remove(selected)\n",
    "        \n",
    "        # Decrement the size of the candidate pool by one\n",
    "        combinedSize -= 1 \n",
    "        \n",
    "    # Return the new population    \n",
    "    return newPopulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we examined the different selection approaches and portions of evolution where selection occurs.  In the next section, we will see how evolutionary algorithms produce new solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4. Reproduction: Where baby solutions come from..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How new solutions are created plays a pivotal role in the performance of evolutionary algorithms.  They define what points in the search space are reachable, how quickly you can traverse the search space, when you converge to a solution, and to where the solutions converges. As mentioned earlier there are two primary means of creating new solutions in evolution: (1) <i>Mutation</i>, and, (2) <i>Recombination</i>.  We shall examine these two concepts in the following sections.  However, first we will modify our evaluator and genome to deal with multiple real-valued numbers as genes in order to demonstrate variations in muation and recombination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a new genome with two\n",
    "# real-valued numbers are genes\n",
    "# with a range of [-3,3]\n",
    "def new_genome():\n",
    "    \n",
    "    # Initialize genes to an empty list\n",
    "    genes = []  \n",
    "    \n",
    "    # Add one value between -3 and 3 as the first gene\n",
    "    genes.append(random.uniform(-3, 3)) \n",
    "    # Add a second value between -3 and 3 as the second gene\n",
    "    genes.append(random.uniform(-3, 3)) \n",
    "    \n",
    "    # Create a new genome\n",
    "    return Genome(-10000, genes)  \n",
    "\n",
    "# Evaluation of the genome is to\n",
    "# maximize a function of two variables\n",
    "def evaluate(genome):\n",
    "    genome.fitness = -math.log(abs((100*(genome.genes[0]**2 - genome.genes[1])**2 + (1 - genome.genes[0])**2)) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation: Copying with error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Mutation</i> is the act of changing the genes of a genome to produce a new genome that is slightly different from the original.  In evolutionary algorithms, this is (mostly) part of <i>Asexual Reproduction</i>, that is, creating a exact clone of a parent genome and then applying mutations to the clone to produce a variant of the parent. To illustrate, we will implement a mutation function for our above multi-gene genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mutates a multiple real-value\n",
    "# genome\n",
    "def mutate_genome(genome):\n",
    "    \n",
    "    # Select one of the real-values uniform randomly\n",
    "    geneToMutateIdx = random.randint(len(genome.genes))\n",
    "    \n",
    "    # Perturbs that value by adding random value in [-0.01, 0.01]\n",
    "    genome.genes[geneToMutateIdx] += random.uniform(-0.01, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above function, it takes in an existing genome (clone of a parent), selects a single gene from among an array of real-valued numbers, and then adds a value between -0.01 and 0.01 to it. Let us look at how this basic mutation performs on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of real-valued numbers, one means of varying how mutation works is by changing the power, or, amount by which the genes are perturbed.  Below, see the effect of turning up the power of the mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mutates a multiple real-value\n",
    "# genome\n",
    "def mutate_genome(genome):\n",
    "    \n",
    "    # Select one of the real-values uniform randomly\n",
    "    geneToMutateIdx = random.randint(len(genome.genes))\n",
    "    \n",
    "    # Perturbs that value by adding random value in [-10, 10]\n",
    "    genome.genes[geneToMutateIdx] += random.uniform(-10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, changing the power of the mutation by switching from a a muation that adds a value in ($-0.01, 0.01$) to a value in ($-10, 10$) significantly alters the algorithms behavior.  Explore how different mutations power can have different impacts by changing the value of <i>power</i> below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "power = 0.1\n",
    "\n",
    "# Mutates a multiple real-value\n",
    "# genome\n",
    "def mutate_genome(genome):\n",
    "    \n",
    "    # Select one of the real-values uniform randomly\n",
    "    geneToMutateIdx = random.randint(len(genome.genes))\n",
    "    \n",
    "    # Perturbs that value by adding random value in [-power, power]\n",
    "    genome.genes[geneToMutateIdx] += random.uniform(-power, power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will perform evolution \n",
    "# with the given parameters\n",
    "def interactive_iteration(generations=1000,\n",
    "                          mu=1,\n",
    "                          lamb=1,\n",
    "                          mutation_power=1.0):\n",
    "    if(mu == 0):\n",
    "        mu = 1\n",
    "    if(lamb == 0):\n",
    "        lamb = 1\n",
    "    global power\n",
    "    power = mutation_power\n",
    "    winner = evolution(mu, lamb, generations)\n",
    "\n",
    "    print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10),\n",
    "                                mutation_power=(0, 10, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related to mutation power, another means of changing mutation is to alter the distribution of mutations selected, that is, we can alter the above function to favor smaller mutations rather than larger mutations.  The above function selects mutations from a uniform distribution, however we could also choose mutations from a normal distribution.  Experiment with the normal distribution below by varying sigma (variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "\n",
    "# Mutates a multiple real-value\n",
    "# genome\n",
    "def mutate_genome(genome):\n",
    "    \n",
    "    # Select one of the real-values uniform randomly\n",
    "    geneToMutateIdx = random.randint(len(genome.genes))\n",
    "    \n",
    "    # Perturbs that value by adding random value in [-power, power]\n",
    "    genome.genes[geneToMutateIdx] += random.normal(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will perform evolution \n",
    "# with the given parameters\n",
    "def interactive_iteration(generations=1000,\n",
    "                          mu=1,\n",
    "                          lamb=1,\n",
    "                          mutation_power=1.0):\n",
    "    if(mu == 0):\n",
    "        mu = 1\n",
    "    if(lamb == 0):\n",
    "        lamb = 1\n",
    "    global power\n",
    "    power = mutation_power\n",
    "    global sigma\n",
    "    sigma = mutation_power\n",
    "    winner = evolution(mu, lamb, generations)\n",
    "\n",
    "    print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10),\n",
    "                                mutation_power=(0, 10, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, another important consideration for mutations is the number of genes mutated per mutation event.  The above mutation functions each select a single gene, at random, and then mutate it.  However, there may be cases where it is advantageous to allow multiple genes to be mutated in a single mutation event.  For example, if you have values $x$ and $y$ into a function $f$, such that $f(x,y + \\Delta y) < f(x,y)$ and $f(x + \\Delta x,y) < f(x,y)$, but $f(x + \\Delta x,y + \\Delta y) > f(x,y)$.  In this case, the only path to optimize the function involves moving $x$ and $y$ at the same time. Thus many approaches to evolutionary computation with multiple genes implement the mutation function such that each gene has a probability of being selected, thereby setting an expected number of genes mutated through this probability.  Below, such an approach is implemented.  Examine the effects as the probability (mutation rate) is raised and lowered by manipulating pMutateGene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pMutateGene = 0.75\n",
    "\n",
    "def mutate_genome(genome):\n",
    "    # For each gene\n",
    "    for x in range(len(genome.genes)): \n",
    "        # Randomly decide if the gene is to be mutated\n",
    "        if(random.uniform(0, 1) < pMutateGene): \n",
    "            # Mutate this gene\n",
    "            genome.genes[x] += random.normal(0, sigma) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function that will perform evolution \n",
    "# with the given parameters\n",
    "def interactive_iteration(generations=1000,\n",
    "                          mu=10,\n",
    "                          lamb=10,\n",
    "                          mutation_power=1.0,\n",
    "                          prob_mutate_gene=0.75):\n",
    "    if(mu == 0):\n",
    "        mu = 1\n",
    "    if(lamb == 0):\n",
    "        lamb = 1\n",
    "    global power\n",
    "    power = mutation_power\n",
    "    global sigma\n",
    "    sigma = mutation_power\n",
    "    global pMutateGene\n",
    "    pMutateGene = prob_mutate_gene\n",
    "    winner = evolution(mu, lamb, generations)\n",
    "\n",
    "    print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10),\n",
    "                                mutation_power=(0, 10, 0.05),\n",
    "                                prob_mutate_gene=(0,1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutations play a very important role in evolutionary algorithms and this section just scratched the surface in the breadth and depth of mutation approaches.  The next section will examine the topic of Recombination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5. Recombination: Playing Legos with genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, evolutionary computation is inspired by the natural process of evolution.  In nature, one method of inducing variation is mutation, as described above. Another approach in nature is for two or more existing genomes to be recombined by taking portions of each to create a new genome.  In this way, promising pieces of each solution can be brought together to make a new, hopefully better, solution.  To this end, we change the reproduce function to create new genomes through recombination and define a recombine function that will take two genomes and return a new genome that is a recombination of the given two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates a new genome by recombining\n",
    "# the genes of the two given parents\n",
    "def recombine(parent1, parent2):\n",
    "    \n",
    "    # Clone the first parent\n",
    "    child = parent1.clone()  \n",
    "    \n",
    "    # Set the second gene of the child to the value \n",
    "    # of the second gene of the second parent\n",
    "    child.genes[1] = parent2.genes[1] \n",
    "    \n",
    "    # Return the child\n",
    "    return child \n",
    "    \n",
    "# Takes in a set of parents and produces an equivalent number of offspring\n",
    "def reproduce(parents): \n",
    " \n",
    "    # Initialize an empty list of offspring\n",
    "    offspring = []\n",
    "    \n",
    "    # For each parent, create offspring through sexual reproduction\n",
    "    for parent in parents: \n",
    "\n",
    "        # Select a second parent\n",
    "        parent2 = parents[random.randint(len(parents))]\n",
    "        \n",
    "        # Create a child by recombining the genes from parent and parent2\n",
    "        child = recombine(parent, parent2)\n",
    "        \n",
    "        # Set its fitness to a very low value\n",
    "        child.fitness = -10000 \n",
    "        \n",
    "        # Add the child to the offspring list\n",
    "        offspring.append(child) \n",
    "        \n",
    "    # Return the list of offspring\n",
    "    return offspring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that through pure recombination, no new genes are introduced.  Instead, whatever we initialize the population with is what we have and evolution can only optimize to the extent that the initial values allow.  Thus the population and offspring size become even more important when dealing with recombination only reproduction.  Explore the effect that switching to recombination has below.  Feel free to explore different evaluation functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(genome):\n",
    "    genome.fitness = -math.log(abs((100*(genome.genes[0]**2 - genome.genes[1])**2 + (1 - genome.genes[0])**2)) + 1)\n",
    "      \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10),\n",
    "                                mutation_power=(0, 10, 0.05),\n",
    "                                prob_mutate_gene=(0,1, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation AND Recombination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us bring it all together by combining <i>mutation</i> and <i>recombination</i>.  Below, we implement a a reproduce method that allows evolution to either produce solutions through mutation or recombination, control by a probability: pRecombination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probability of performing\n",
    "# recombination rather than mutation\n",
    "pRecombination = 0.5\n",
    "\n",
    "# Takes in a set of parents and produces \n",
    "# an equivalent number of offspring\n",
    "def reproduce(parents):\n",
    "    \n",
    "    # Initialize an empty list of offspring\n",
    "    offspring = [] \n",
    "    \n",
    "    # For each parent, create offspring \n",
    "    for parent in parents: \n",
    "        child = None\n",
    "        \n",
    "        # If we reproduce through cloning and mutation\n",
    "        if(random.uniform(0, 1) > pRecombination):\n",
    "            # Clone the parent\n",
    "            child = parent.clone();\n",
    "            # Mutate the clone\n",
    "            mutate_genome(child)\n",
    "        # Else we reproduce through recombination\n",
    "        else:\n",
    "            # Select a second parent\n",
    "            parent2 = parents[random.randint(len(parents))]\n",
    "            # Create a child by recombining the genes from parent and parent2\n",
    "            child = recombine(parent, parent2) \n",
    "            \n",
    "        # Set child's fitness to a very low value\n",
    "        child.fitness = -10000 \n",
    "        # Add the child to the offspring list\n",
    "        offspring.append(child) \n",
    "    \n",
    "    # Return the list of offspring\n",
    "    return offspring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a simple algorithm that implements the primary features of evolutionary approaches.  Experiment with different parameter settings and evaluation functions!  We will explore advanced topics in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(genome):\n",
    "    genome.fitness = -math.log(abs((100*(genome.genes[0]**2 - genome.genes[1])**2 + (1 - genome.genes[0])**2)) + 1)\n",
    "\n",
    "    # Function that will perform evolution \n",
    "# with the given parameters\n",
    "def interactive_iteration(generations=1000,\n",
    "                          mu=10,\n",
    "                          lamb=10,\n",
    "                          mutation_power=1.0,\n",
    "                          prob_mutate_gene=0.75,\n",
    "                          prob_recombine=0.5):\n",
    "    if(mu == 0):\n",
    "        mu = 1\n",
    "    if(lamb == 0):\n",
    "        lamb = 1\n",
    "    global power\n",
    "    power = mutation_power\n",
    "    global sigma\n",
    "    sigma = mutation_power\n",
    "    global pMutateGene\n",
    "    pMutateGene = prob_mutate_gene\n",
    "    global pRecombine\n",
    "    pRecombine = prob_recombine\n",
    "    winner = evolution(mu, lamb, generations)\n",
    "\n",
    "    print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))\n",
    "    \n",
    "# Create interactive sliders for parameters\n",
    "interact(interactive_iteration, generations=(0,10000,10),\n",
    "                                mu=(0,1000, 10),\n",
    "                                lamb=(0,1000, 10),\n",
    "                                mutation_power=(0, 10, 0.05),\n",
    "                                prob_mutate_gene=(0,1, 0.05),\n",
    "                                prob_recombine=(0,1,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. (Optional) Advanced Topics: Theory, How to make policies, Co-Evolution, and Prison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are briefly going to cover basic theory of how evolutionary algorithms work and then discuss how evolution can search through policies and the idea of co-evolution through the Prisoner's Dilemma domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. Building blocks and Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine some basic theory underlying evolutionary algorithms, we are going to implement a genetic algorithm type approach (i.e. evolving strings of binary valued bits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of bits in the genome\n",
    "numBits = 32 \n",
    "# Probability of mutating a bit\n",
    "pMutateGene = 1.0/numBits\n",
    "# Probability of recombining two genomes\n",
    "pRecombination = 0.5 \n",
    "# Population Size\n",
    "mu = 10 \n",
    "# Offspring size\n",
    "lamb = 10 \n",
    "\n",
    "# Creates and returns a new random genome\n",
    "def new_genome(): \n",
    "    bits = []\n",
    "    for x in range(numBits):\n",
    "        bits.append(random.randint(2))\n",
    "    # A random genome for this domain is array of 0's and 1's\n",
    "    return Genome(-10000, bits) \n",
    "\n",
    "# Mutation takes a genome and alters \n",
    "# its genes to create a new solution instance\n",
    "def mutate_genome(genome): \n",
    "    # For the number genes\n",
    "    for x in range(len(genome.genes)): \n",
    "        # If randomly selected number is \n",
    "        # below probability of mutating single gene\n",
    "        if(random.uniform(0, 1) < pMutateGene): \n",
    "            # Mutate this gene by flipping 0 to 1, or, 0 to 1\n",
    "            genome.genes[x] = (genome.genes[x] + 1) % 2 \n",
    "\n",
    "# Creates offspring by recombining\n",
    "# the genes of the given parents\n",
    "def recombine(parent1, parent2):  \n",
    "    # Clone parent 1\n",
    "    child = parent1.clone()\n",
    "    # Choose a point where the child's genes \n",
    "    # start to come from parent 2\n",
    "    crossoverPoint = random.randint(1, len(child.genes)-1) \n",
    "    # Set the child genes to parent 1's genes\n",
    "    # up to the crossover point\n",
    "    child.genes = child.genes[:crossoverPoint]\n",
    "    # Complete the child's genes with parent 2's\n",
    "    # genes after the crossover point\n",
    "    child.genes.extend(parent2.genes[crossoverPoint:])\n",
    "    # Return the child\n",
    "    return child \n",
    "\n",
    "# Evaluator that is the OneMax Domain\n",
    "def evaluate(genome):\n",
    "    genome.fitness = 0\n",
    "    # Implementation of OneMax domain \n",
    "    # (i.e. maximize the number of 1 bits in the genome)\n",
    "    # Sums the number of bits set to 1\n",
    "    for x in genome.genes: \n",
    "        genome.fitness += x\n",
    "\n",
    "winner = evolution(mu, lamb, 1000)\n",
    "print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code implements the OneMax domain for genetic algorithms.  This is a very simple domain that provides a sanity check to make sure your algorithm is working and a platform for exploring properties of evolutionary algorithms.  In this example, the genome consists of an array of bits, that is, an array of values that are either 0 or 1.  The evaluator determines fitness by counting the number of array values that are set to 1.  Thus the optimal array is filled with 1's. Note: The recombine method above performs <i>single-point crossover</i> (i.e. the recombination is performed by splitting the parent arrays at a single point).  As an exercise, try implementing multi-point crossover, wherein recombination splits the genes array at multiple points to create a child.\n",
    "\n",
    "The OneMax domain allows us to explain the concept of 'building blocks'.  A hypothetical basis for the performance of evolution is building blocks, that is, beneficial genes will begin to occur together and form into modules.  These beneficial groupings will then propagate throughout the system and combine with other beneficial groupings to make even better solutions.  This 'building blocks' hypothesis was formally defined by John Holland (1975) in the Schema Theorem for genetic algorithms.\n",
    "\n",
    "A schema is template of a bit string consisting of <i>defined</i> bits (0, 1), or <i>wild cards</i> (\\*).  Thus a particular schema may be $H = 1***1$, meaning the schema is 5 bits, beginning and ending with 1 and the rest we don't care.  The idea is that such schemas represent building blocks throughout a genetic algorithm.  If a particular schema on average contributes to more fit genomes, they will then begin occuring more frequently in the system, that is, schemas are <i>implicitly</i> evaluated through the evolutionary process.  Because each genome will represent multiple schema at any given time, this gives rise to the idea of <i>implicit parallelism</i>, wherein many schemas are being evaluated simultaneously.\n",
    "\n",
    "This is the extent to which we will cover the theoretical foundations, however such theories allow mathematical analysis of the effect that selection, recombination, and mutation have through the effect such actions have on schema.  The next section we will explore how evolution can create agent policies and the idea of co-evolution through game theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2. The Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate learning policies, we introduce the well known Prisoner's Dilemma problem from Game Theory.  In the Prisoner's Dilemma, two criminal conspiritors are caught by the police and imprisoned.  The prisoners are separated into two different interrogation rooms.  Now, the police do not have enough evidence to convict them on the full charges of their crimes, so they need confessions.  To each prisoner the police offer the same deal: Rat out your partner and we'll give you a lighter sentence.  If neither defects on their partner, they both serve a short prison sentence.  If only one defects, the defector gets the minimum sentence, while the one who remains silent receives a very long sentence.  Finally, if both defect, they both get long sentences.  Thus defecting, which is the path to the minimum sentence, can result in a worse outcome than if both stayed silent, thereby creating the dilemna.  Can the prisoners trust each other to not go for the minimum sentence? \n",
    "\n",
    "Formally, you have two agents that can either cooperate (C), or defect (D), and the reward structure for an individual agent is such that DC > CC > DD > CD, where the first letter the agent's action and the second letter is the co-agent's action.  Such a reward structure can be envisioned by the following matrix: \n",
    "\\begin{array}{ccc}\n",
    " & C & D \\\\\n",
    "C & 3,3 & 0,5 \\\\\n",
    "D & 5,0 & 1,1 \\end{array}\n",
    "A more interesting extension, called the Iterated Prisoner's Dilemna, extends the game to multiple moves, that is, the agents have to play the game several times in a row and the rewards accumulate across all the decisions and the agents are aware of the results of the previous games.  Thus if your opponent always defects, you can respond in kind, rather than always losing.  \n",
    "\n",
    "A challenge for learning policies to such competitive domains is evaluation.  For example, if we wanted to learn policies to play chess, we would need an already built system that can play chess... but if we have such a system, why do we need to learn one?  Additionally, if the competitor that is evaluated against is too good, all policies will initially perform equally poorly against it making learning difficult.  Finally, the policies that are learned will be optimized to play against that particular opponent rather than an arbitrary opponent. Addressing these concerns is the field of <i>Co-evolution</i>.  In co-evolution, instead of an oracle evaluator that tells us the exact fitness of each individual, individuals compete (or cooperate) together to determine their fitness.  Thus, going back to the chess example, genomes within the population would compete against each other in chess games to determine how well they perform.  The hope would there would be an 'arms race' or Red Queen effect, wherein genomes must continually improve in performance to maintain fitness relative to the other individuals in the population. Below we modify our evolutionary approach to implement co-evolution to learn policies for the Iterated Prisoner's Dilemna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evolution(populationSize, offspringSize, maxGenerations):\n",
    "    \n",
    "    currentGeneration = 0 # Current generation being executed\n",
    "    \n",
    "    avgFitnessByGeneration = [] # Array to hold generational average fitness values\n",
    "    bestFitnessByGeneration = [] # Array to hold generational champion fitness values\n",
    "    championValuesByGeneration = [] # Array to hold generational champion values\n",
    "    \n",
    "    population = []  # Initialize an empty population\n",
    "    bestGenome = None # Set the best genome to a null value\n",
    "    \n",
    "    avg = 0.0  # Initialize variable for calculating population average fitness to 0\n",
    "    for x in range(populationSize): # For the size of the population\n",
    "        population.append(new_genome()) # Createa new random genome and add it to the population\n",
    "    for x in range(populationSize): # For the size of the population\n",
    "        evaluate(population[x], population) # Evaluate each genome against the population\n",
    "        avg += population[x].fitness # Add its fitness score to the average\n",
    "        if(bestGenome is None or population[x].fitness > bestGenome.fitness):  # If the best genome hasn't been set, or, this\n",
    "                                                                               # genome has better fitness\n",
    "            bestGenome = population[x] # Set the bestGenome to this genome\n",
    "    \n",
    "    # Add the metrics to their various arrays for tracking\n",
    "    avgFitnessByGeneration.append(avg/populationSize)\n",
    "    bestFitnessByGeneration.append(bestGenome.fitness)\n",
    "    championValuesByGeneration.append(bestGenome.genes)\n",
    "    \n",
    "    while(currentGeneration < maxGenerations): # For the maximum number of generations\n",
    "        \n",
    "        parents = select_parents(population, offspringSize) # Select a number of parents equal to the number of offspring to create\n",
    "        offspring = reproduce(parents) # Create a number of offspring by reproducing the selected parents\n",
    "        bestGenome = None\n",
    "        for child in offspring: # For each of the offspring\n",
    "            evaluate(child, population) # Evaluate the offspring against the existing population\n",
    "            if(bestGenome is None or child.fitness > bestGenome.fitness): # Update the best genome is applicable\n",
    "                bestGenome = child\n",
    "        \n",
    "        population = select_population(population, offspring) # Select a new population from the current population and offspring\n",
    "        \n",
    "        avg = 0.0 # Init average to 0\n",
    "        for genome in population: # For each genome in the population\n",
    "            avg += genome.fitness # Add its fitness to the average\n",
    "   \n",
    "        # Add the metrics to the various arrays\n",
    "        avgFitnessByGeneration.append(avg/populationSize)\n",
    "        bestFitnessByGeneration.append(bestGenome.fitness)\n",
    "        championValuesByGeneration.append(bestGenome.genes)\n",
    "        \n",
    "        currentGeneration += 1 # Increment current generation by one\n",
    "    \n",
    "    # Plot the best fitness, average fitness, and the champion gene values by generation\n",
    "    labels = range(len(bestFitnessByGeneration))\n",
    "    subplot(2,1,1)\n",
    "    plot(labels, bestFitnessByGeneration, labels, avgFitnessByGeneration)\n",
    "    ylabel(\"Fitness\")\n",
    "    legend([\"Best\", \"Avg.\"], loc='lower center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "    subplot(2,1,2)\n",
    "    plot(labels, championValuesByGeneration)\n",
    "    ylabel(\"Gene Values\")\n",
    "    \n",
    "    \n",
    "    return bestGenome # Return the best genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numBits = 5  # Five bits to represent the prisoner's policy, with meaning defect, and 0 meaning cooperate\n",
    "             # The first bit represents an action to take with no history, followed by history of CC, DC, CD, and DD\n",
    "rewards = [3, 5, 0, 1] # Rewards for CC, DC, CD, DD\n",
    "gameIterations = 100\n",
    "\n",
    "def dilemma(policy, opponent, iterations): # Score a given policy against an opponent for a number of iterations\n",
    "    score = 0.0 # Initial score of 0\n",
    "    lastOutcome = 0 # Last outcome with (0 = No history, 1 = CC, 2 = DC, 3 = CD, 4 = DD)\n",
    "    for x in range(iterations): # For the number of iterations\n",
    "        policyDecision = policy[lastOutcome] # Get the policy's decision given last outcome\n",
    "        opponentDecision = opponent[lastOutcome] # Get opponent's decision given last outcome\n",
    "        lastOutcome = 1 + policyDecision * 1 + opponentDecision * 2 # Set the last outcome\n",
    "        score += rewards[lastOutcome - 1] # Get the score given the outcome\n",
    "    return score / iterations # Dive the score by number of iterations\n",
    "\n",
    "def evaluate(genome, opponents):\n",
    "    genome.fitness = 0\n",
    "    for x in opponents: # For each opponent, evaluate the genome against it\n",
    "        genome.fitness += dilemma(genome.genes, x.genes, gameIterations)\n",
    "    genome.fitness = genome.fitness / len(opponents) # Divide fitness by number of opponents for an average    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several modifications can be seen in the above code.  For the evolutionary loop, we now pass in a set of opponents to evaluate a particular genome against for the evaluate function.  The evaluate function now takes a genome and a set of opponents and evaluates that genomes policy against each of these opponents and averages the performance.  The dilemma functions performds the Iterated Prisoner's Dilemma between a given policy and an opponent for a number of iterations.  Experiment below with different parameters and observe the significant difference that can occur in a co-evolutionary fitness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = 100 # Population Size\n",
    "lamb = 10 # Offspring size\n",
    "totalGenerations = 1000 # Number of generations of evolution \n",
    "pMutateGene = 1.0/numBits # Probability of flipping a bit\n",
    "pRecombination = 0 # Probability of recombining genomes rather than mutating\n",
    "rewards = [3, 5, 0, 1] # Rewards for CC, DC, CD, DD\n",
    "gameIterations = 100 # Number of iterations for the Prisoner's Dilemma\n",
    "\n",
    "winner = evolution(mu, lamb, totalGenerations)\n",
    "print(\"Champion -- Genes = \" + str(winner.genes) + \" Fitness = \" + str(winner.fitness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Evolutionary Computation Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ends our introduction to evolutionary computation.  We completed an overview of a basic evolutionary algorithm, demonstrated the key components of the algorithm and the effects they have, and discussed soem preliminaries on theory and applications to policy search for reinforcement learning.  As a reminder, the key parts to implementing the evolutionary algorithms are:\n",
    "\n",
    "<ul>\n",
    "<li><b>Genomes:</b> Representations of problem solutions</li>\n",
    "<li><b>Evaluation:</b> Means of determining genome's fitness</li>\n",
    "<li><b>Reproduction:</b> Process by which existing genomes (<b>parents</b>) produce new genomes (<b>offspring, or children</b>), through <b>mutation</b> or <b>recombination</b></li>\n",
    "<li><b>Selection:</b> A process by which either (1) parents are chosen to create offspring, or, (2) individuals are chosen to die and be replaced </li>\n",
    "</ul>\n",
    "\n",
    "There is a vast amount of literature out there if you wish to learn more, from basic text books like <i>Evolutionary Computation: a unified approach</i> by Kenneth De Jong and <i>An Introduction to Genetic Algorithms</i> by Melanie Mitchell, to cutting edge international conferences, such as ACM's Genetic and Evolutionary Computation Conference (GECCO) and IEEE's Congress on Evolutionary Computation (CEC).  \n",
    "\n",
    "Evolutionary computation has (pardon the pun) evolved over time from its simple optimization roots, to increasingly complex areas, such as swarm intelligence and artificial life.  There remains debate whether evolutionary algorithms are reinforcement learning; however, there is little debate that problems that reinforcement learning address can often also be solved by evolutionary approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
